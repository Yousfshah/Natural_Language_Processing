{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90808d43",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .highlight-text {\n",
    "        font-family: 'Arial', sans-serif;\n",
    "        font-size: 3em; /* Increased for heading prominence */\n",
    "        font-weight: bold;\n",
    "        background: linear-gradient(45deg, #3498db, #8e44ad);\n",
    "        -webkit-background-clip: text;\n",
    "        -webkit-text-fill-color: transparent;\n",
    "        text-shadow: 3px 3px 6px rgba(0, 0, 0, 0.3); /* Enhanced text shadow */\n",
    "        display: block;\n",
    "        text-align: center; /* Center the text */\n",
    "        padding: 15px 20px; /* Increased padding for heading feel */\n",
    "        border-radius: 10px;\n",
    "        background-color: #153160ff; /* Retained background color */\n",
    "        box-shadow: 0 4px 12px 4px rgba(0, 0, 0); /* Added box shadow for depth */\n",
    "        margin: 20px auto; /* Centered with auto margins */\n",
    "        max-width: 80%; /* Prevents overly wide heading */\n",
    "        transition: transform 0.3s ease, box-shadow 0.3s ease; /* Smooth hover effects */\n",
    "    }\n",
    "\n",
    "    .highlight-text:hover {\n",
    "        transform: scale(1.05); /* Subtle zoom on hover */\n",
    "        box-shadow: 0 6px 16px rgba(0, 0, 0, 0.3); /* Enhanced shadow on hover */\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<span class=\"highlight-text\">Text Preprocessing</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc27726",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 42rem; margin: 2.5rem auto; padding: 2rem; text-align: center; background: linear-gradient(135deg, #8d8d8dff, #9c9b9bff); box-shadow: 0 0.5rem 1.25rem rgba(0, 0, 0, 0.15); font-family: 'Lato', Arial, sans-serif; border:5px solid black\">\n",
    "\n",
    "  <h3 style=\"font-size: 1.5rem; font-weight: 700; margin-bottom: 1.5rem; color: #000000; letter-spacing: 0.05em;\">\n",
    "    <strong style=\"color: #000;\">Author Name:</strong>\n",
    "    <a href=\"https://www.linkedin.com/in/yousuf-shah-7ba9492b4/\" style=\"color: #fff; text-decoration: none; font-weight: bold;\">Yousuf Shah</a>\n",
    "  </h3>\n",
    "\n",
    "  <div style=\"display: flex; flex-wrap: wrap; justify-content: center; gap: 1.25rem;\">\n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/yousuf-shah-7ba9492b4/\" style=\"text-decoration: none;\">\n",
    "  <img src=\"https://img.shields.io/badge/LinkedIn-Connect-0a66c2?style=for-the-badge&logo=linkedin&logoColor=white\" alt=\"LinkedIn\" style=\"height: 3.3rem; padding: 0.3rem; background: #000;\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://yousfshah.github.io/Portfolio_Website/\" style=\"text-decoration: none;\">\n",
    "  <img src=\"https://img.shields.io/badge/Portfolio-Visit-ffb300?style=for-the-badge&logo=firefox&logoColor=white\" alt=\"Portfolio\" style=\"height: 3.3rem; padding: 0.3rem; background: #000;\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://github.com/Yousfshah\" style=\"text-decoration: none;\">\n",
    "  <img src=\"https://img.shields.io/badge/GitHub-Explore-6f42c1?style=for-the-badge&logo=github&logoColor=white\" alt=\"GitHub\" style=\"height: 3.3rem; padding: 0.3rem; background: #000;\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.kaggle.com/yousufshah\" style=\"text-decoration: none;\">\n",
    "  <img src=\"https://img.shields.io/badge/Kaggle-Profile-0097b2?style=for-the-badge&logo=kaggle&logoColor=white\" alt=\"Kaggle\" style=\"height: 3.3rem; padding: 0.3rem; background: #000;\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://yousfshah.github.io/Blogging/\" style=\"text-decoration: none;\">\n",
    "  <img src=\"https://img.shields.io/badge/Blog-Articles-f44336?style=for-the-badge&logo=blogger&logoColor=white\" alt=\"Blog\" style=\"height: 3.3rem; padding: 0.3rem; background: #000;\">\n",
    "</a>\n",
    "\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910a6b4",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 2.5rem auto; padding: 2rem 2.5rem; background: linear-gradient(135deg, #8d8d8dff, #9c9b9bff); box-shadow: 0 0.75rem 1.5rem rgba(0, 0, 0, 0.08); font-family: 'Segoe UI', 'Lato', sans-serif; color: #111; border-left: 6px solid #0055ff; border-radius: 0.5rem;\">\n",
    "\n",
    "  <h3 style=\"font-size: 1.8rem; font-weight: 700; margin-bottom: 1.2rem; color: #000000ff;\">\n",
    "    üìå Why You Should Practice This Notebook\n",
    "  </h3>\n",
    "\n",
    "  <p style=\"font-size: 1.05rem; line-height: 1.7; margin-bottom: 1rem; color:#000; font-weight: bold;\">\n",
    "    If you're serious about mastering NLP, this notebook gives you a hands-on, end-to-end experience with real-world text preprocessing ‚Äî the foundation of every NLP project.\n",
    "    You'll learn to clean, normalize, and prepare raw text using both industry-standard tools (<strong>spaCy</strong> + <strong>NLTK</strong>) and best practices followed in real ML pipelines.\n",
    "  </p>\n",
    "\n",
    "  <ul style=\"font-size: 1.05rem; line-height: 1.7; padding-left: 1.2rem; margin-bottom: 1rem; color:#000; font-weight: bold;\">\n",
    "    <li>‚úÖ Practical code examples</li>\n",
    "    <li>‚úÖ Reusable functions</li>\n",
    "    <li>‚úÖ Cleaned input ready for ML, BERT, or vectorization</li>\n",
    "  </ul>\n",
    "\n",
    "  <p style=\"font-size: 1.05rem; line-height: 1.7; font-weight: 600; color: #ab1700ff;\">\n",
    "    Good models start with great preprocessing.<br>\n",
    "  </p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f888d2",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ **Lowercasing**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Words like \"Good\", \"GOOD\", and \"good\" are semantically identical but are treated as separate tokens if not lowercased.\n",
    "\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In a product review system, \"Excellent\" and \"excellent\" should be considered the same sentiment word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6cbb363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  Good Morning EVERYONE! Let's Start Our NLP Journey. \n",
      "\n",
      "üü¢ Lowercased Text:  good morning everyone! let's start our nlp journey.\n"
     ]
    }
   ],
   "source": [
    "def to_lowercase(text):\n",
    "    # Convert text to lowercase\n",
    "    if not isinstance(text, str):\n",
    "        print(\"‚ö†Ô∏è Input must be a string.\")\n",
    "        return None\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"Good Morning EVERYONE! Let's Start Our NLP Journey.\"\n",
    "lowercased = to_lowercase(text)\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Lowercased Text: \", lowercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a091a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f3aa0",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ **Remove HTML Tags**\n",
    "\n",
    "### üîç **Why:**\n",
    "- HTML is markup used for layout, not meaning.\n",
    "\n",
    "- If you're scraping websites (news, blogs), tags like div tag, span tag add noise.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- When extracting articles from news sites, you'll find lots of formatting tags. Models get confused by p tag, a href tag etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec120d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  <div>Hello <b>World</b>! NLP is <i>awesome</i>.</div> \n",
      "\n",
      "üü¢ Cleaned with BS4:  Hello World! NLP is awesome. \n",
      "\n",
      "üî¥ Cleaned with Regex:  Hello World! NLP is awesome.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "\n",
    "def remove_html_tags(text, method=\"bs4\"):\n",
    "    # Remove HTML tags using BeautifulSoup or regex\n",
    "    if not isinstance(text, str):\n",
    "        print(\"‚ö†Ô∏è Input must be a string.\")\n",
    "        return None\n",
    "\n",
    "    if method == \"bs4\":\n",
    "        return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    elif method == \"regex\":\n",
    "        pattern = r\"<(?:\\\"[^\\\"]*\\\"['\\\"]*|'[^']*'['\\\"]*|[^'\\\">])+>\"\n",
    "        return re.sub(pattern, '', text)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Invalid method. Use 'bs4' or 'regex'.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "html_text = \"<div>Hello <b>World</b>! NLP is <i>awesome</i>.</div>\"\n",
    "\n",
    "cleaned_bs4 = remove_html_tags(html_text, method=\"bs4\")\n",
    "cleaned_regex = remove_html_tags(html_text, method=\"regex\")\n",
    "\n",
    "print(\"üîµ Original Text: \", html_text, \"\\n\")\n",
    "print(\"üü¢ Cleaned with BS4: \", cleaned_bs4, \"\\n\")\n",
    "print(\"üî¥ Cleaned with Regex: \", cleaned_regex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f8ff7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316925a",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ **Remove URLs**\n",
    "\n",
    "### üîç **Why:**\n",
    "- URLs usually don‚Äôt carry semantic meaning.\n",
    "\n",
    "- They are high variance strings (each one is unique) and hurt model generalization.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In a tweet like ‚ÄúCheck this out üëâ https://xyz.com‚Äù, we care more about the sentiment or emotion, not the URL itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95d0ffdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  Visit our website at https://www.example.com/page.html or check out www.another-site.org. You can also find info at mydomain.net/info. \n",
      "\n",
      "üü¢ Text Without URL:  Visit our website at  or check out  You can also find info at \n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "def remove_urls(text):\n",
    "    # Remove all types of URLs from text\n",
    "    if not isinstance(text, str):\n",
    "        print(\"‚ö†Ô∏è Input must be a string.\")\n",
    "        return None\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r'https?://[^\\s/$.?#].[^\\s]*'             # http/https\n",
    "        r'|www\\.[^\\s/$.?#].[^\\s]*'                # www.\n",
    "        r'|[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}(/[^\\s]*)?' # domain.com\n",
    "    )\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"Visit our website at https://www.example.com/page.html or check out www.another-site.org. You can also find info at mydomain.net/info.\"\n",
    "\n",
    "cleaned_text = remove_urls(text)\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Text Without URL: \", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d70b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752634f5",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ **Remove Punctuation**\n",
    "\n",
    "### üîç **Why:**\n",
    "-  Punctuation marks like `!`, `.`, `?` create unnecessary tokens.\n",
    "\n",
    "- Removing them helps reduce noise, especially in tasks like text classification or topic modeling.\n",
    "\n",
    "| Method                               | Removes ASCII | Removes Unicode | Customizable |\n",
    "| ------------------------------------ | ------------- | --------------- | ------------ |\n",
    "| `string.punctuation` + `translate()` | ‚úÖ Yes         | ‚ùå No            | ‚ùå Limited    |\n",
    "| `re.sub(r'\\p{P}+', '', text)`       | ‚úÖ Yes         | ‚úÖ Yes           | ‚úÖ Yes        |\n",
    "\n",
    "- string.punctuation only covers ASCII punctuation & returns: <div style=color:red;>!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~</div>\n",
    "\n",
    "- Unicode symbols like  `‚Äú`, `‚Äî`, and `‚Ä¶` are not included in string.punctuation, so we need to add them manually or using regex\n",
    "\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- For spam detection or document classification, punctuation doesn‚Äôt usually help (unless you‚Äôre analyzing writing style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b850b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  Hello!!! How are you??? I'm fine:) ‚ÄúGreat‚Äù‚Äî yes‚Ä¶ \n",
      "\n",
      "üü¢ Text Without Punctuation:  Hello How are you Im fine Great yes\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Remove punctuation from text\n",
    "    if not isinstance(text, str):\n",
    "        print(\"‚ö†Ô∏è Input must be a string.\")\n",
    "        return None\n",
    "\n",
    "    custom_punct = string.punctuation + '‚Äú‚Äù‚Äî‚Ä¶'\n",
    "    return text.translate(str.maketrans('', '', custom_punct))\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello!!! How are you??? I'm fine:) ‚ÄúGreat‚Äù‚Äî yes‚Ä¶\"\n",
    "cleaned_text = remove_punctuation(text)\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Text Without Punctuation: \", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8e9421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  Hello!!! How are you??? I'm fine:) ‚ÄúGreat‚Äù‚Äî yes‚Ä¶ \n",
      "\n",
      "üü¢ Text Without Punctuation (Regex):  Hello How are you Im fine Great yes\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "def remove_punctuation_regex(text):\n",
    "    # Remove all punctuation (Unicode-aware)\n",
    "    if not isinstance(text, str):\n",
    "        print(\"‚ö†Ô∏è Input must be a string.\")\n",
    "        return None\n",
    "\n",
    "    return re.sub(r'\\p{P}+', '', text)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello!!! How are you??? I'm fine:) ‚ÄúGreat‚Äù‚Äî yes‚Ä¶\"\n",
    "cleaned_text = remove_punctuation_regex(text)\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Text Without Punctuation (Regex): \", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e21be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5094c72",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ **Chat Word Treatment (e.g., GN ‚Üí Good Night)**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Chat language is full of abbreviations: \"lol\", \"idk\", \"smh\".\n",
    "\n",
    "- They need to be normalized to standard English for models to understand.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In customer support or social media, replacing abbreviations like `\"brb\"` with `\"be right back\"` helps understand the message better.\n",
    "\n",
    "### üó£Ô∏è **Slang Words**\n",
    "- The following three GitHub repositories provide comprehensive lists of slang words and their standard English equivalents.These resources are useful for expanding chat abbreviations (e.g., \"GN\" ‚Üí \"Good Night\") during text preprocessing\n",
    "\n",
    "    - [Repo1](https://github.com/ipekdk/abbreviation-list-english)\n",
    "    - [Repo2](https://github.com/bodhwani/NLP-VIT-BOT/blob/master/slangs.csv)\n",
    "    - [Comman Slang Words](https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79551381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  BRB guys, lol this is funny. ttyl! \n",
      "\n",
      "üü¢ Text After Slang Replacement:  Be Right Back guys , Laughing Out Loud this is funny . Talk To You Later !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "def replace_slangs(text, slang_csv_path):\n",
    "    # Replace slangs using a CSV dictionary and spaCy tokenization\n",
    "    if not isinstance(text, str):\n",
    "        print(\"‚ö†Ô∏è Input must be a string.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        slang_df = pd.read_csv(slang_csv_path)\n",
    "        slang_map = dict(zip(slang_df['Abbr'].str.lower(), slang_df['Fullform']))\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error loading slang dictionary:\", e)\n",
    "        return None\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        word = token.text\n",
    "        lower = word.lower()\n",
    "        if lower in slang_map:\n",
    "            tokens.extend(slang_map[lower].split())\n",
    "        else:\n",
    "            tokens.append(word)\n",
    "\n",
    "    return \" \".join(tokens) # The join() method takes all items in an iterable and joins them into one string. A string must be specified as the separator\n",
    "\n",
    "# Example usage\n",
    "text = \"BRB guys, lol this is funny. ttyl!\"\n",
    "slang_csv = \"../Extra_Materials/slangs.csv\"\n",
    "\n",
    "processed = replace_slangs(text, slang_csv)\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Text After Slang Replacement: \", processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c83a6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af2dbc",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ **Spelling Correction**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Typos are common in user input (especially social media).\n",
    "\n",
    "- Words like ‚Äúbeleive‚Äù won‚Äôt match ‚Äúbelieve‚Äù in dictionaries or embeddings.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- Spell correction boosts chatbot understanding and auto-correction in search bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0145f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  I realy love natral language prosesing. \n",
      "\n",
      "üü¢ Autocorrect:  I really love natural language crossing. \n",
      "\n",
      "üî¥ TextBlob:  I really love natural language pressing. \n",
      "\n",
      "üü° PySpellChecker:  I really love natural language prosesing.\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "def correct_spelling(text, method='autocorrect'):\n",
    "    # Correct spelling using the selected method\n",
    "    if not isinstance(text, str):\n",
    "        print(\"‚ö†Ô∏è Input must be a string.\")\n",
    "        return None\n",
    "\n",
    "    if method == 'autocorrect':\n",
    "        spell = Speller(lang='en')\n",
    "        return spell(text)\n",
    "\n",
    "    elif method == 'textblob':\n",
    "        return str(TextBlob(text).correct())\n",
    "\n",
    "    elif method == 'pyspellchecker':\n",
    "        spellchecker = SpellChecker()\n",
    "        tokens = text.split()\n",
    "        return ' '.join([spellchecker.correction(word) or word for word in tokens])\n",
    "\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Invalid method. Choose from: 'autocorrect', 'textblob', 'pyspellchecker'\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"I realy love natral language prosesing.\"\n",
    "\n",
    "auto = correct_spelling(text, method='autocorrect')\n",
    "blob = correct_spelling(text, method='textblob')\n",
    "pyspell = correct_spelling(text, method='pyspellchecker')\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Autocorrect: \", auto, \"\\n\")\n",
    "print(\"üî¥ TextBlob: \", blob, \"\\n\")\n",
    "print(\"üü° PySpellChecker: \", pyspell)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e06efd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f78e50",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ **Remove Stop Words**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Stopwords like \"the\", \"is\", \"at\" are function words, not content words.\n",
    "\n",
    "- Removing them improves focus on important tokens.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In a movie review, ‚ÄúThe movie was absolutely fantastic‚Äù, word ‚Äúfantastic‚Äù carries the sentiment, not ‚Äúthe‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5532e1f",
   "metadata": {},
   "source": [
    "#### **Remove stop words with the help of spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe1c6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  He must be going to the market because he has to buy some fruits and vegetables. \n",
      "\n",
      "üü¢ Text Without Stop Words:  going market buy fruits vegetables .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Remove stop words using spaCy\n",
    "    if not isinstance(text, str):\n",
    "        print(\"‚ö†Ô∏è Input must be a string.\")\n",
    "        return None\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.text for token in doc if not token.is_stop]) # is_stop is a boolean attribute of the token object that indicates whether the token is a stop word\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"He must be going to the market because he has to buy some fruits and vegetables.\"\n",
    "cleaned = remove_stopwords(text)\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Text Without Stop Words: \", cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02af8bf",
   "metadata": {},
   "source": [
    "#### **Remove stop words with the help of NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe556a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  He must be going to the market because he has to buy some fruits and vegetables. \n",
      "\n",
      "üü¢ Text Without Stop Words:  must going market buy fruits vegetables .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords_nltk(text):\n",
    "    # Remove stop words using NLTK\n",
    "    if not isinstance(text, str):\n",
    "        print(\"‚ö†Ô∏è Input must be a string.\")\n",
    "        return None\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"He must be going to the market because he has to buy some fruits and vegetables.\"\n",
    "cleaned = remove_stopwords_nltk(text)\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Text Without Stop Words: \", cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee8450",
   "metadata": {},
   "source": [
    "### **Why Differences Between NLTK and spaCy Stop Word Removal**\n",
    "| **Aspect**                          | **NLTK**                                                                  | **spaCy**                                                          |\n",
    "| ----------------------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------ |\n",
    "| **Stop Word List Size**             | Smaller list (about \\~179 words in English)                               | Larger list (around \\~326 words in `en_core_web_sm`)               |\n",
    "| **Tokenization Dependency**         | Uses `word_tokenize()` ‚Äî may split contractions (`can't` ‚Üí `ca`, `n't`)   | Uses `spaCy` tokenizer ‚Äî more robust with contractions and symbols |\n",
    "| **Customization Needed?**           | Often requires manual addition/removal to match task requirements         | More complete by default but still may require fine-tuning         |\n",
    "| **Default Case Sensitivity**        | Case-sensitive (you must lowercase tokens before comparison)              | Handles case-insensitivity internally with `token.is_stop`         |\n",
    "| **Performance**                     | Lightweight, fast                                                         | Slightly slower due to full NLP pipeline                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc3974",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680edd1",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ **Handling Emojis**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Emojis are sentiment-rich tokens (üòä, üò¢).\n",
    "\n",
    "- You can either remove them, convert to words, or use them as features.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In social media sentiment analysis, üò† and ‚ù§Ô∏è change the tone completely and must be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2e03834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text: Good job!üëçI‚Äôm so happyüòä\n",
      "\n",
      "üü¢ Text With Handled Emojis: Good job! thumbs up I‚Äôm so happy smiling face with smiling eyes \n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import regex as re\n",
    "\n",
    "text = \"Good job!üëçI‚Äôm so happyüòä\"\n",
    "emoji_text = emoji.demojize(text)\n",
    "\n",
    "# Remove ':' and '_' characters\n",
    "emoji_text_cleaned = emoji_text.replace(':', ' ').replace('_', ' ')\n",
    "\n",
    "print(f\"üîµ Original Text: {text}\\n\")\n",
    "print(f\"üü¢ Text With Handled Emojis: {emoji_text_cleaned}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee66ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f43ef",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ **Tokenization**\n",
    "\n",
    "### ‚ö†Ô∏è **Very Important Step**\n",
    "\n",
    "### üîç **Why:**\n",
    "Machine learning models in NLP typically require numerical input. Tokenization converts text into a format that can be converted into numerical representations (like word embeddings) that models can understand.\n",
    "\n",
    "#### **Example:**\n",
    "Let's say we have the sentence: `The quick brown fox jumps over the lazy dog.`\n",
    "Word Tokenization:\n",
    "> [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In the search engine, \"best Italian restaurants near me\" is tokenized into the words: [\"best\", \"Italian\", \"restaurants\", \"near\", \"me\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a10828",
   "metadata": {},
   "source": [
    "#### Tokenization with `spacy.blank(\"en\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce2221d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  Hello, world! This is a test sentence. Let's see how it works. \n",
      "\n",
      "üü¢ After Tokenization Using spacy.blank('en'):  ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', '.', 'Let', \"'s\", 'see', 'how', 'it', 'works', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Create a blank English NLP object (only tokenizer) No lemmatization, NER, POS & Tagging\n",
    "nlp_blank = spacy.blank(\"en\")\n",
    "\n",
    "# Input text\n",
    "text = \"Hello, world! This is a test sentence. Let's see how it works.\"\n",
    "\n",
    "# Tokenize using blank pipeline\n",
    "doc_blank = nlp_blank(text)\n",
    "\n",
    "# Print each token\n",
    "token_list = [token.text for token in doc_blank]\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(f\"üü¢ After Tokenization Using spacy.blank('en'): \", token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f7c66",
   "metadata": {},
   "source": [
    "#### Tokenization with `spacy.load(\"en_core_web_sm\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e65bf405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  Hello, world! This is a test sentence. Let's see how it works. \n",
      "\n",
      "üü¢ After Tokenization Using English Model:  ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', '.', 'Let', \"'s\", 'see', 'how', 'it', 'works', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load pretrained small English model (includes tokenizer, POS, NER, etc.)\n",
    "nlp_full = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input text\n",
    "text = \"Hello, world! This is a test sentence. Let's see how it works.\"\n",
    "\n",
    "# Process text\n",
    "doc_full = nlp_full(text)\n",
    "\n",
    "# Print each token\n",
    "token_list = [token.text for token in doc_full]\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ After Tokenization Using English Model: \", token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b63df1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e6015",
   "metadata": {},
   "source": [
    "## üîü **Stemming**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Stemming reduces words to a base/root form. It‚Äôs fast and works well in information retrieval systems.\n",
    "\n",
    "- May produce non-words (e.g., ‚Äústudies‚Äù ‚Üí ‚Äústudi‚Äù).\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- Used in search engines (e.g., ‚Äúsearching‚Äù, ‚Äúsearched‚Äù, ‚Äúsearches‚Äù ‚Üí ‚Äúsearch‚Äù) to show relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bed7ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  ['Organizing', 'events', 'helps', 'communities', 'grow', 'and', 'stay', 'connected', '.'] \n",
      "\n",
      "üü¢ After Stemming Using PorterStemmer:  ['organ', 'event', 'help', 'commun', 'grow', 'and', 'stay', 'connect', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Load spaCy (for tokenization)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Three well-known stemming algorithms are the Porter stemmer, Snowball stemmer, and Lancaster stemmer\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Input text\n",
    "text = \"Organizing events helps communities grow and stay connected.\"\n",
    "\n",
    "# Tokenize using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Apply stemming using NLTK\n",
    "stemmed_tokens = [stemmer.stem(token.text) for token in doc]\n",
    "\n",
    "# Output\n",
    "print(\"üîµ Original Text: \", [token.text for token in doc], \"\\n\")\n",
    "print(\"üü¢ After Stemming Using PorterStemmer: \", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67c16e",
   "metadata": {},
   "source": [
    "#### **The 3 Main NLTK Stemmers + Lemmatization**\n",
    "\n",
    "| Technique            | Aggressiveness   | Language Support                 | Speed                     | Output Quality               | Notes                                                               |\n",
    "| -------------------- | ---------------- | -------------------------------- | ------------------------- | ---------------------------- | ------------------------------------------------------------------- |\n",
    "| **PorterStemmer**    | ‚úÖ Moderate       | English only                     | ‚úÖ Fast                    | ‚úÖ Good, widely used          | Most commonly used, balanced stemming algorithm                     |\n",
    "| **LancasterStemmer** | üî• Very high     | English only                     | ‚úÖ Fast                    | ‚ùå Over-stems words           | Very aggressive ‚Üí `maximum` ‚Üí `maxim`                       |\n",
    "| **SnowballStemmer**  | üü¢ Balanced      | ‚úÖ Multiple langs                 | ‚úÖ Fast                    | ‚úÖ Clean, improved            | Also called ‚ÄúPorter2‚Äù ‚Äî better, more consistent than Porter         |\n",
    "| **Lemmatization**    | ‚ùå Not aggressive | ‚úÖ Multiple langs (WordNet/spaCy) | ‚ö†Ô∏è Slower (context-aware) | ‚úÖ Real words & context-aware | Uses grammar + vocabulary, returns meaningful dictionary base forms |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dd86751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  The organization organized organized events efficiently. \n",
      "\n",
      "üü¢ PorterStemmer:  ['the', 'organ', 'organ', 'organ', 'event', 'effici', '.'] \n",
      "\n",
      "üî¥ LancasterStemmer:  ['the', 'org', 'org', 'org', 'ev', 'efficy', '.'] \n",
      "\n",
      "üü° SnowballStemmer:  ['the', 'organ', 'organ', 'organ', 'event', 'effici', '.'] \n",
      "\n",
      "üü£ Lemmatization:  ['the', 'organization', 'organize', 'organize', 'event', 'efficiently', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"The organization organized organized events efficiently.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = nlp(text)\n",
    "\n",
    "# Initialize stemmers\n",
    "porter = PorterStemmer() # Balanced stemmer\n",
    "lancaster = LancasterStemmer() # aggressive stemmer\n",
    "snowball = SnowballStemmer(\"english\") # Also Called Porter2\n",
    "\n",
    "# Apply each stemmer\n",
    "porter_result = [porter.stem(word.text) for word in tokens]\n",
    "lancaster_result = [lancaster.stem(word.text) for word in tokens]\n",
    "snowball_result = [snowball.stem(word.text) for word in tokens]\n",
    "lemmized_result = [word.lemma_ for word in tokens]  # Lemmatization using spaCy\n",
    "\n",
    "print(\"üîµ Original Text: \", tokens, \"\\n\")\n",
    "print(\"üü¢ PorterStemmer: \", porter_result, \"\\n\")\n",
    "print(\"üî¥ LancasterStemmer: \", lancaster_result, \"\\n\")\n",
    "print(\"üü° SnowballStemmer: \", snowball_result, \"\\n\")\n",
    "print(\"üü£ Lemmatization: \", lemmized_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8fc55b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c15c0",
   "metadata": {},
   "source": [
    "## üî¢ **Lemmatization**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Like stemming, but linguistically accurate.\n",
    "\n",
    "- Uses vocabulary and grammar rules to return the correct base word.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- For grammar-sensitive tasks like question answering or summarization, ‚Äúwas‚Äù should be lemmatized to ‚Äúbe‚Äù, not ‚Äúwa‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2227650",
   "metadata": {},
   "source": [
    "#### **Comparison: Lemmatization in spaCy vs NLTK**\n",
    "| Feature           | **spaCy**                         | **NLTK (WordNetLemmatizer)**         |\n",
    "| ----------------- | --------------------------------- | ------------------------------------ |\n",
    "| Accuracy          | ‚úÖ High (uses POS + context)       | ‚ö†Ô∏è Medium (needs manual POS mapping) |\n",
    "| Language model    | `en_core_web_sm` or larger models | WordNet dictionary                   |\n",
    "| POS Tag Awareness | ‚úÖ Automatic                       | ‚ö†Ô∏è Must provide POS manually         |\n",
    "| Output Quality    | ‚úÖ Real, accurate base forms       | ‚úÖ Good, but context-blind sometimes  |\n",
    "| Ease of Use       | ‚úÖ Very easy                       | ‚ö†Ô∏è Slightly complex (POS conversion) |\n",
    "| Performance       | ‚úÖ Fast and optimized              | ‚úÖ Fast but rule-based                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "489ea81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  The organization organized organized events efficiently. \n",
      "\n",
      "üü¢ After Lemmatization:  the child be run and well organize event every week . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"The children were running and better organized events every week.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Lemmatize with POS awareness\n",
    "lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "# Output\n",
    "print(\"üîµ Original Text: \", tokens, \"\\n\")\n",
    "print(\"üü¢ After Lemmatization: \", \" \".join(lemmatized_tokens), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b0caad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Lemmas: ['The', 'child', 'be', 'run', 'and', 'good', 'organize', 'event', 'every', 'week', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# POS tag conversion\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "text = \"The children were running and better organized events every week.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "result = [tokens.text for tokens in doc]\n",
    "tagged = pos_tag(result)\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n",
    "print(\"NLTK Lemmas:\", lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2105d408",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d646831",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary Table\n",
    "\n",
    "| Step                | Purpose                                                                 | Use Case Examples                                     |\n",
    "|---------------------|-------------------------------------------------------------------------|--------------------------------------------------------|\n",
    "| Lowercasing          | Normalize text casing                                                   | Text classification, search                           |\n",
    "| Remove HTML Tags     | remove unwanted HTML tags                                                | Web scraping, email cleaning                          |\n",
    "| Remove URLs          | Remove unnecessary web links                                             | Social media, forums                                  |\n",
    "| Remove Punctuation   | Clean punctuation noise                                                  | Preprocessing for BoW/TF-IDF                          |\n",
    "| Chat Word Treatment  | Convert slang to standard English like GD to Good Night                  | Chatbots, social media analysis                       |\n",
    "| Spelling Correction  | Fix typos for vocabulary consistency                                     | User reviews, text input                              |\n",
    "| Remove Stop Words    | Focus on meaningful words                                                 | Summarization, topic modeling                         |\n",
    "| Handle Emojis        | Preserve or convert emojis based on task                                | Sentiment analysis                                    |\n",
    "| Tokenization         | Break down text into tokens                                              | All NLP tasks                                         |\n",
    "| Stemming             | Reduce word forms to base/root                                           | Search engines, topic modeling                        |\n",
    "| Lemmatization        | Get accurate root word using grammar                                     | Parsing, QA systems, deep learning models             |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
