{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8b6d61",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f888d2",
   "metadata": {},
   "source": [
    "## 1️⃣ **Lowercasing**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Words like \"Good\", \"GOOD\", and \"good\" are semantically identical but are treated as separate tokens if not lowercased.\n",
    "\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In a product review system, \"Excellent\" and \"excellent\" should be considered the same sentiment word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbb363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: Good Morning EVERYONE! Let's Start Our NLP Journey. \n",
      "\n",
      "🟢 Preprocessed: good morning everyone! let's start our nlp journey.\n"
     ]
    }
   ],
   "source": [
    "text = \"Good Morning EVERYONE! Let's Start Our NLP Journey.\"\n",
    "lowercased = text.lower()\n",
    "\n",
    "print(\"🔵 Original:\", text, \"\\n\")\n",
    "print(\"🟢 Preprocessed:\", lowercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a091a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f3aa0",
   "metadata": {},
   "source": [
    "## 2️⃣ **Remove HTML Tags**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- HTML is markup used for layout, not meaning.\n",
    "\n",
    "- If you're scraping websites (news, blogs), tags like div tag, span tag add noise.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- When extracting articles from news sites, you'll find lots of formatting tags. Models get confused by p tag, a href tag etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: <div>Hello <b>World</b>! NLP is <i>awesome</i>.</div> \n",
      "\n",
      "🟢 Preprocessed with BS4: Hello World! NLP is awesome. \n",
      "\n",
      "🟢 Preprocessed with regex: Hello World! NLP is awesome.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# sample HTML text\n",
    "text = \"<div>Hello <b>World</b>! NLP is <i>awesome</i>.</div>\"\n",
    "\n",
    "# Using BeautifulSoup to remove HTML tags\n",
    "cleaned = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# Using regex to remove HTML tags\n",
    "re_pattern = \"<(?:\\\"[^\\\"]*\\\"['\\\"]*|'[^']*'['\\\"]*|[^'\\\">])+>\" # # Remove all HTML tags from a string\n",
    "cleaned_re = re.sub(re_pattern, '', text)\n",
    "# cleaned = cleaned.strip()\n",
    "\n",
    "print(\"🔵 Original:\", text, \"\\n\")\n",
    "print(\"🟢 Preprocessed with BS4:\", cleaned, \"\\n\")\n",
    "print(\"🟢 Preprocessed with regex:\", cleaned_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f8ff7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316925a",
   "metadata": {},
   "source": [
    "## 3️⃣ **Remove URLs**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- URLs usually don’t carry semantic meaning.\n",
    "\n",
    "- They are high variance strings (each one is unique) and hurt model generalization.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In a tweet like “Check this out 👉 https://xyz.com”, we care more about the sentiment or emotion, not the URL itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0ffdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: Visit our website at https://www.example.com/page.html or check out www.another-site.org. You can also find info at mydomain.net/info. \n",
      "\n",
      "🟢 Preprocessed: Visit our website at  or check out  You can also find info at \n"
     ]
    }
   ],
   "source": [
    "# it will remove any type of URL from the text just apply on your text\n",
    "import re\n",
    "\n",
    "# Example usage:\n",
    "text = \"Visit our website at https://www.example.com/page.html or check out www.another-site.org. You can also find info at mydomain.net/info.\"\n",
    "\n",
    "# Regex pattern to match various URL formats (http, https, www, without scheme)\n",
    "url_pattern = re.compile(\n",
    "    r'https?://[^\\s/$.?#].[^\\s]*'  # Matches http/https URLs\n",
    "    r'|www\\.[^\\s/$.?#].[^\\s]*'    # Matches www. URLs\n",
    "    r'|[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}(?:/[^\\s]*)?' # Matches domain-only URLs like example.com\n",
    ")\n",
    "\n",
    "# Remove URLs from the text\n",
    "text_without_urls= url_pattern.sub(r'', text)\n",
    "\n",
    "print(\"🔵 Original:\", text, \"\\n\")\n",
    "print(\"🟢 Preprocessed:\", text_without_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d70b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752634f5",
   "metadata": {},
   "source": [
    "## 4️⃣ **Remove Punctuation**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "-  Punctuation marks like `!`, `.`, `?` create unnecessary tokens.\n",
    "\n",
    "- Removing them helps reduce noise, especially in tasks like text classification or topic modeling.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- For spam detection or document classification, punctuation doesn’t usually help (unless you’re analyzing writing style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b850b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: (Hello!!! How are you??? I'm fine :) \n",
      "\n",
      "🟢 Preprocessed: Hello How are you Im fine \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = \"Hello!!! How are you??? I'm fine :)\"\n",
    "no_punc = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "print(\"🔵 Original:\", text, \"\\n\")\n",
    "print(\"🟢 Preprocessed:\", no_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e21be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5094c72",
   "metadata": {},
   "source": [
    "## 5️⃣ **Chat Word Treatment (e.g., GN → Good Night)**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Chat language is full of abbreviations: \"lol\", \"idk\", \"smh\".\n",
    "\n",
    "- They need to be normalized to standard English for models to understand.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In customer support or social media, replacing abbreviations like `\"brb\"` with `\"be right back\"` helps understand the message better.\n",
    "\n",
    "### 🗣️ **Slang Words**\n",
    "- The following two GitHub repositories provide comprehensive lists of slang words and their standard English equivalents.These resources are useful for expanding chat abbreviations (e.g., \"GN\" → \"Good Night\") during text preprocessing\n",
    "\n",
    "    - [Repo1](https://github.com/ipekdk/abbreviation-list-english)\n",
    "    - [Repo2](https://github.com/bodhwani/NLP-VIT-BOT/blob/master/slangs.csv)\n",
    "    - [Comman Slang Words](https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55030324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: BRB guys, lol this is funny. ttyl! \n",
      "\n",
      "🟢 Preprocessed: Be Right Back guys, Laughing Out Loud this is funny. Talk To You Later\n"
     ]
    }
   ],
   "source": [
    "# just apply to your text this comprehenisve list of slang abbreviations\n",
    "# Make sure to have a CSV file named 'slangs.csv' with columns 'Abbr\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the slang abbreviation CSV file\n",
    "chat_dict = pd.read_csv(\"slangs.csv\")\n",
    "\n",
    "# Create a mapping dictionary for fast lookup\n",
    "slang_map = dict(zip(chat_dict['Abbr'].str.lower(), chat_dict['Fullform']))\n",
    "\n",
    "text = \"BRB guys, lol this is funny. ttyl!\"\n",
    "words = text.split()\n",
    "chat_fixed = []\n",
    "for word in words:\n",
    "    key = word.lower().strip(\".,!?\")  # Remove punctuation for matching\n",
    "    if key in slang_map:\n",
    "        chat_fixed.append(slang_map[key])\n",
    "    else:\n",
    "        chat_fixed.append(word)\n",
    "\n",
    "print(\"🔵 Original:\", text, \"\\n\")\n",
    "print(\"🟢 Preprocessed:\", ''.join(chat_fixed)) # The join() method takes all items in an iterable and joins them into one string. A string must be specified as the separator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c83a6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af2dbc",
   "metadata": {},
   "source": [
    "## 6️⃣ **Spelling Correction**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Typos are common in user input (especially social media).\n",
    "\n",
    "- Words like “beleive” won’t match “believe” in dictionaries or embeddings.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- Spell correction boosts chatbot understanding and auto-correction in search bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0145f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: I realy love natral language prosesing. \n",
      "\n",
      "🟢 Preprocessed (autocorrect): I really love natural language crossing. \n",
      "\n",
      "🟢 Preprocessed (TextBlob): I really love natural language pressing. \n",
      "\n",
      "🟢 Preprocessed (pyspellchecker): I really love natural language prosesing.\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "text = \"I realy love natral language prosesing.\"\n",
    "\n",
    "# Autocorrect\n",
    "corrected_autocorrect = spell(text)\n",
    "\n",
    "# TextBlob\n",
    "corrected_textblob = str(TextBlob(text).correct())\n",
    "\n",
    "# SpellChecker\n",
    "spellchecker = SpellChecker()\n",
    "# Tokenize the text for word-level correction\n",
    "tokens = text.split()\n",
    "corrected_pyspellchecker = ' '.join([spellchecker.correction(word) or word for word in tokens])\n",
    "\n",
    "\n",
    "print(\"🔵 Original:\", text, \"\\n\")\n",
    "print(\"🟢 Preprocessed (autocorrect):\", corrected_autocorrect, \"\\n\")\n",
    "print(\"🟢 Preprocessed (TextBlob):\", corrected_textblob, \"\\n\")\n",
    "print(\"🟢 Preprocessed (pyspellchecker):\", corrected_pyspellchecker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e06efd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f78e50",
   "metadata": {},
   "source": [
    "## 7️⃣ **Remove Stop Words**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Stopwords like \"the\", \"is\", \"at\" are function words, not content words.\n",
    "\n",
    "- Removing them improves focus on important tokens.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In a movie review, “The movie was absolutely fantastic”, word “fantastic” carries the sentiment, not “the”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b648d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: This is an example of removing common stop words.\n",
      "🟢 NLTK (no stopwords): realy love natral language prosesing.\n",
      "🟢 spaCy (no stopwords): example removing common stop words .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "# NLTK stopword removal\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "nltk_filtered = [word for word in tokens if word.lower() not in nltk_stopwords]\n",
    "nltk_no_stopwords = \" \".join(nltk_filtered)\n",
    "\n",
    "# spaCy stopword removal\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_doc = nlp(text)\n",
    "spacy_filtered = [token.text for token in spacy_doc if not token.is_stop]\n",
    "spacy_no_stopwords = \" \".join(spacy_filtered)\n",
    "\n",
    "print(\"🔵 Original:\", text)\n",
    "print(\"🟢 NLTK (no stopwords):\", nltk_no_stopwords)\n",
    "print(\"🟢 spaCy (no stopwords):\", spacy_no_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dc1e6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english') # You can specify other languages\n",
    "type(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61979021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yousuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Yousuf/nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Yousuf\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Download the punkt tokenizer models if not already present\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# The variables stop_words and text are already defined in the notebook\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m words = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenize the text into words\u001b[39;00m\n\u001b[32m      8\u001b[39m filtered_words = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word.lower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[32m      9\u001b[39m filtered_text = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(filtered_words)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Yousuf/nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Yousuf\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download the punkt tokenizer models if not already present\n",
    "\n",
    "# The variables stop_words and text are already defined in the notebook\n",
    "\n",
    "words = word_tokenize(text)  # Tokenize the text into words\n",
    "\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "filtered_text = \" \".join(filtered_words)\n",
    "\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f2c8dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yousuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yousuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Yousuf/nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Yousuf\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mThis is an example of removing common stop words.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m filtered = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word.lower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m     12\u001b[39m no_stopwords = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(filtered)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Yousuf/nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Yousuf\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Ensure required NLTK resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"This is an example of removing common stop words.\"\n",
    "tokens = word_tokenize(text)\n",
    "filtered = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "no_stopwords = \" \".join(filtered)\n",
    "\n",
    "print(\"🔵 Original:\", text)\n",
    "print(\"🟢 Preprocessed:\", no_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd6e4b1",
   "metadata": {},
   "source": [
    "| Feature       | NLTK                             | spaCy                                   |\n",
    "| ------------- | -------------------------------- | --------------------------------------- |\n",
    "| Stopword List | Manually maintained              | Linguistically curated                  |\n",
    "| Tokenization  | Basic (word\\_tokenize)           | Advanced (handles context, punctuation) |\n",
    "| Flexibility   | High (can customize list easily) | High (custom rules, pipelines)          |\n",
    "| Performance   | Lightweight                      | Heavier but more powerful               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc3974",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680edd1",
   "metadata": {},
   "source": [
    "## 8️⃣ **Handling Emojis**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Emojis are sentiment-rich tokens (😊, 😢).\n",
    "\n",
    "- You can either remove them, convert to words, or use them as features.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In social media sentiment analysis, 😠 and ❤️ change the tone completely and must be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e03834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "text = \"Good job! 👍 I’m so happy 😊\"\n",
    "emoji_text = emoji.demojize(text)\n",
    "\n",
    "print(\"🔵 Original:\", text)\n",
    "print(\"🟢 Preprocessed:\", emoji_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee66ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f43ef",
   "metadata": {},
   "source": [
    "## 9️⃣ **Tokenization**\n",
    "\n",
    "### ⚠️ **Very Important Step**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "Machine learning models in NLP typically require numerical input. Tokenization converts text into a format that can be converted into numerical representations (like word embeddings) that models can understand.\n",
    "\n",
    "#### **Example:**\n",
    "Let's say we have the sentence: `The quick brown fox jumps over the lazy dog.`\n",
    "Word Tokenization:\n",
    "> [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In the search engine, \"best Italian restaurants near me\" is tokenized into the words: [\"best\", \"Italian\", \"restaurants\", \"near\", \"me\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenization splits sentences into words.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"🔵 Original:\", text)\n",
    "print(\"🟢 Preprocessed:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b63df1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e6015",
   "metadata": {},
   "source": [
    "## 🔟 **Stemming**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Stemming reduces words to a base/root form. It’s fast and works well in information retrieval systems.\n",
    "\n",
    "- May produce non-words (e.g., “studies” → “studi”).\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- Used in search engines (e.g., “searching”, “searched”, “searches” → “search”) to show relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b3186",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "text = \"We are studying various stemmed words: running, studies, cries\"\n",
    "tokens = word_tokenize(text)\n",
    "stemmed = [stemmer.stem(w) for w in tokens]\n",
    "\n",
    "print(\"🔵 Original:\", tokens)\n",
    "print(\"🟢 Preprocessed:\", stemmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8fc55b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c15c0",
   "metadata": {},
   "source": [
    "## 🔢 **Lemmatization**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Like stemming, but linguistically accurate.\n",
    "\n",
    "- Uses vocabulary and grammar rules to return the correct base word.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- For grammar-sensitive tasks like question answering or summarization, “was” should be lemmatized to “be”, not “wa”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98194606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"The children were playing with their better toys.\"\n",
    "doc = nlp(text)\n",
    "lemmatized = [token.lemma_ for token in doc]\n",
    "\n",
    "print(\"🔵 Original:\", text)\n",
    "print(\"🟢 Preprocessed:\", lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042e66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d646831",
   "metadata": {},
   "source": [
    "## ✅ Summary Table\n",
    "\n",
    "| Step                | Purpose                                                                 | Use Case Examples                                     |\n",
    "|---------------------|-------------------------------------------------------------------------|--------------------------------------------------------|\n",
    "| Lowercasing          | Normalize text casing                                                   | Text classification, search                           |\n",
    "| Remove HTML Tags     | remove unwanted HTML tags                                                | Web scraping, email cleaning                          |\n",
    "| Remove URLs          | Remove unnecessary web links                                             | Social media, forums                                  |\n",
    "| Remove Punctuation   | Clean punctuation noise                                                  | Preprocessing for BoW/TF-IDF                          |\n",
    "| Chat Word Treatment  | Convert slang to standard English like GD to Good Night                  | Chatbots, social media analysis                       |\n",
    "| Spelling Correction  | Fix typos for vocabulary consistency                                     | User reviews, text input                              |\n",
    "| Remove Stop Words    | Focus on meaningful words                                                 | Summarization, topic modeling                         |\n",
    "| Handle Emojis        | Preserve or convert emojis based on task                                | Sentiment analysis                                    |\n",
    "| Tokenization         | Break down text into tokens                                              | All NLP tasks                                         |\n",
    "| Stemming             | Reduce word forms to base/root                                           | Search engines, topic modeling                        |\n",
    "| Lemmatization        | Get accurate root word using grammar                                     | Parsing, QA systems, deep learning models             |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
