{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90808d43",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .highlight-text {\n",
    "        font-family: 'Arial', sans-serif;\n",
    "        font-size: 3em; /* Increased for heading prominence */\n",
    "        font-weight: bold;\n",
    "        background: linear-gradient(45deg, #3498db, #8e44ad);\n",
    "        -webkit-background-clip: text;\n",
    "        -webkit-text-fill-color: transparent;\n",
    "        text-shadow: 3px 3px 6px rgba(0, 0, 0, 0.3); /* Enhanced text shadow */\n",
    "        display: block;\n",
    "        text-align: center; /* Center the text */\n",
    "        padding: 15px 20px; /* Increased padding for heading feel */\n",
    "        border-radius: 10px;\n",
    "        background-color: #153160ff; /* Retained background color */\n",
    "        box-shadow: 0 4px 12px 4px rgba(0, 0, 0); /* Added box shadow for depth */\n",
    "        margin: 20px auto; /* Centered with auto margins */\n",
    "        max-width: 80%; /* Prevents overly wide heading */\n",
    "        transition: transform 0.3s ease, box-shadow 0.3s ease; /* Smooth hover effects */\n",
    "    }\n",
    "\n",
    "    .highlight-text:hover {\n",
    "        transform: scale(1.05); /* Subtle zoom on hover */\n",
    "        box-shadow: 0 6px 16px rgba(0, 0, 0, 0.3); /* Enhanced shadow on hover */\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<span class=\"highlight-text\">Text Preprocessing</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3a973",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 650px; margin: 40px auto; padding: 25px; border-radius: 10px; text-align: center; background: linear-gradient(135deg, #000000ff, #000000ff); border: 2px solid #ffffff; box-shadow: 0 6px 20px rgba(0, 0, 0, 0.15);\" onmouseover=\"this.style.transform='translateY(-3px)'; this.style.boxShadow='0 8px 25px rgba(0, 0, 0, 0.2)'\" onmouseout=\"this.style.transform='translateY(0)'; this.style.boxShadow='0 6px 20px rgba(0, 0, 0, 0.15)'\">\n",
    "  <h3 style=\"font-size: 24px; font-weight: 700; color: #212121; margin-bottom: 20px; font-family: 'Lato', Arial, sans-serif; letter-spacing: 1px;\">\n",
    "    <strong style = color:white>Author Name:</strong>\n",
    "    <a href=\"https://www.linkedin.com/in/yousuf-shah-7ba9492b4/\" target=\"_blank\" style=\"color: #daff06ff; text-decoration: none; font-weight: bold;\">Yousuf Shah</a>\n",
    "  </h3>\n",
    "  <table style=\"margin: 0 auto; border-spacing: 20px;\">\n",
    "    <tr>\n",
    "      <td>\n",
    "        <a href=\"https://www.linkedin.com/in/yousuf-shah-7ba9492b4/\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/LinkedIn-Profile-blue?style=for-the-badge&logo=linkedin\" alt=\"LinkedIn\" style=\"height: 55px; border-radius: 8px; background: linear-gradient(45deg, #000000, #000000); padding: 5px;\"/>\n",
    "        </a>\n",
    "      </td>\n",
    "      <td>\n",
    "        <a href=\"https://yousfshah.github.io/Portfolio_Website/\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/Portfolio-Website-blue?style=for-the-badge&logo=link\" alt=\"Portfolio Website\" style=\"height: 55px; border-radius: 8px; background: linear-gradient(45deg, #000000, #000000); padding: 5px;\"/>\n",
    "        </a>\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\n",
    "        <a href=\"https://github.com/Yousfshah\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/GitHub-Profile-green?style=for-the-badge&logo=github\" alt=\"GitHub\" style=\"height: 55px; border-radius: 8px; background: linear-gradient(45deg, #000000, #000000); padding: 5px;\"/>\n",
    "        </a>\n",
    "      </td>\n",
    "      <td>\n",
    "        <a href=\"https://www.kaggle.com/yousufshah\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/Kaggle-Profile-orange?style=for-the-badge&logo=kaggle\" alt=\"Kaggle\" style=\"height: 55px; border-radius: 8px; background: linear-gradient(45deg, #000000, #000000); padding: 5px;\"/>\n",
    "        </a>\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        <a href=\"https://yousfshah.github.io/Blogging/\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/Blog-Posts-purple?style=for-the-badge&logo=blogger\" alt=\"Blog\" style=\"height: 55px; border-radius: 8px; background: linear-gradient(45deg, #000000, #000000); padding: 5px;\"/>\n",
    "        </a>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48d9b3d",
   "metadata": {},
   "source": [
    "<style>\n",
    "  /* General Reset */\n",
    "  * {\n",
    "    margin: 0;\n",
    "    padding: 0;\n",
    "    box-sizing: border-box;\n",
    "  }\n",
    "\n",
    "  /* Contact Info Section */\n",
    "  .contact-info {\n",
    "    max-width: 700px; /* Slightly wider for better spacing */\n",
    "    margin: 30px auto;\n",
    "    padding: 25px;\n",
    "    border-radius: 20px;\n",
    "    text-align: center;\n",
    "    background: linear-gradient(135deg, #ffffffff, #000d0cff); /* Dark gradient for modern look */\n",
    "    border: 3px solid #000000ff; /* Vibrant border */\n",
    "    box-shadow: 0 8px 25px rgba(0, 0, 0, 0.5), inset 0 0 10px rgba(255, 255, 255, 0.2); /* 3D effect */\n",
    "    transition: transform 0.3s ease, box-shadow 0.3s ease;\n",
    "  }\n",
    "\n",
    "  .contact-info:hover {\n",
    "    transform: translateY(-5px); /* Lift effect on hover */\n",
    "    box-shadow: 0 12px 30px rgba(119, 0, 0, 0.6), inset 0 0 15px rgba(122, 0, 86, 0.3);\n",
    "  }\n",
    "\n",
    "  /* Section Titles */\n",
    "  .section-title {\n",
    "    font-size: 24px;\n",
    "    font-weight: bold;\n",
    "    color: #0a0101ff; /* White for contrast */\n",
    "    margin-bottom: 20px;\n",
    "    text-shadow: 0 0 8px rgba(2, 62, 19, 0.7); /* Glowing effect */\n",
    "    font-family: 'Roboto', Arial, sans-serif;\n",
    "  }\n",
    "\n",
    "  .section-title a {\n",
    "    color: #02303cff; /* Vibrant cyan */\n",
    "    text-decoration: none;\n",
    "    transition: color 0.3s ease, text-shadow 0.3s ease;\n",
    "  }\n",
    "\n",
    "  .section-title a:hover {\n",
    "    color: #340000ff; /* Coral on hover */\n",
    "    text-shadow: 0 0 12px rgba(84, 0, 76, 0.8);\n",
    "  }\n",
    "\n",
    "  /* Table for Social Links */\n",
    "  .contact-info table {\n",
    "    margin: 0 auto;\n",
    "    border-spacing: 20px; /* Increased for better separation */\n",
    "  }\n",
    "\n",
    "  /* Social Media Badges */\n",
    "  .contact-info img {\n",
    "    height: 60px; /* Slightly larger badges */\n",
    "    border-radius: 12px;\n",
    "    transition: all 0.3s ease;\n",
    "    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3); /* Initial shadow */\n",
    "    background: linear-gradient(45deg, #000000ff, #4d4d4dff); /* Card-like background */\n",
    "    padding: 5px; /* Subtle padding for card effect */\n",
    "  }\n",
    "\n",
    "  .contact-info img:hover {\n",
    "    transform: scale(1.2) rotate(5deg); /* Scale and slight rotation */\n",
    "    box-shadow: 0 8px 20px rgba(0, 0, 0, 0.5), 0 0 15px rgba(176, 192, 0, 0.7); /* Glowing shadow */\n",
    "    background: linear-gradient(45deg, #2b0353ff, #610012ff); /* Gradient on hover */\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<div class=\"contact-info\">\n",
    "  <h3 class=\"section-title\">\n",
    "    <strong>Author Name:</strong>\n",
    "    <a href=\"https://www.linkedin.com/in/yousuf-shah-7ba9492b4/\" target=\"_blank\">Yousuf Shah</a>\n",
    "  </h3>\n",
    "  <table>\n",
    "    <tr>\n",
    "      <td>\n",
    "        <a href=\"https://www.linkedin.com/in/yousuf-shah-7ba9492b4/\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/LinkedIn-Profile-blue?style=for-the-badge&logo=linkedin\" alt=\"LinkedIn\" />\n",
    "        </a>\n",
    "      </td>\n",
    "      <td>\n",
    "        <a href=\"https://yousfshah.github.io/Portfolio_Website/\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/Portfolio_Website-Website-blue?style=for-the-badge&logo=link\" alt=\"Portfolio Website\" />\n",
    "        </a>\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\n",
    "        <a href=\"https://github.com/Yousfshah\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/GitHub-Profile-green?style=for-the-badge&logo=github\" alt=\"GitHub\" />\n",
    "        </a>\n",
    "      </td>\n",
    "      <td>\n",
    "        <a href=\"https://www.kaggle.com/yousufshah\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/Kaggle-Profile-orange?style=for-the-badge&logo=kaggle\" alt=\"Kaggle\" />\n",
    "        </a>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910a6b4",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 2.5rem auto; padding: 2rem 2.5rem; background: linear-gradient(135deg, #8d8d8dff, #9c9b9bff); box-shadow: 0 0.75rem 1.5rem rgba(0, 0, 0, 0.08); font-family: 'Segoe UI', 'Lato', sans-serif; color: #111; border-left: 6px solid #0055ff; border-radius: 0.5rem;\">\n",
    "\n",
    "  <h3 style=\"font-size: 1.8rem; font-weight: 700; margin-bottom: 1.2rem; color: #000000ff;\">\n",
    "    📌 Why You Should Practice This Notebook\n",
    "  </h3>\n",
    "\n",
    "  <p style=\"font-size: 1.05rem; line-height: 1.7; margin-bottom: 1rem; color:#000; font-weight: bold;\">\n",
    "    If you're serious about mastering NLP, this notebook gives you a hands-on, end-to-end experience with real-world text preprocessing — the foundation of every NLP project.\n",
    "    You'll learn to clean, normalize, and prepare raw text using both industry-standard tools (<strong>spaCy</strong> + <strong>NLTK</strong>) and best practices followed in real ML pipelines.\n",
    "  </p>\n",
    "\n",
    "  <ul style=\"font-size: 1.05rem; line-height: 1.7; padding-left: 1.2rem; margin-bottom: 1rem; color:#000; font-weight: bold;\">\n",
    "    <li>✅ Practical code examples</li>\n",
    "    <li>✅ Reusable functions</li>\n",
    "    <li>✅ Cleaned input ready for ML, BERT, or vectorization</li>\n",
    "  </ul>\n",
    "\n",
    "  <p style=\"font-size: 1.05rem; line-height: 1.7; font-weight: 600; color: #ab1700ff;\">\n",
    "    Good models start with great preprocessing.<br>\n",
    "  </p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f888d2",
   "metadata": {},
   "source": [
    "## 1️⃣ **Lowercasing**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Words like \"Good\", \"GOOD\", and \"good\" are semantically identical but are treated as separate tokens if not lowercased.\n",
    "\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In a product review system, \"Excellent\" and \"excellent\" should be considered the same sentiment word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6cbb363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original Text:  Good Morning EVERYONE! Let's Start Our NLP Journey. \n",
      "\n",
      "🟢 Lower Case Text:  good morning everyone! let's start our nlp journey.\n"
     ]
    }
   ],
   "source": [
    "text = \"Good Morning EVERYONE! Let's Start Our NLP Journey.\"\n",
    "lowercased = text.lower()\n",
    "\n",
    "print(\"🔵 Original Text: \", text, \"\\n\")\n",
    "print(\"🟢 Lower Case Text: \", lowercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a091a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f3aa0",
   "metadata": {},
   "source": [
    "## 2️⃣ **Remove HTML Tags**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- HTML is markup used for layout, not meaning.\n",
    "\n",
    "- If you're scraping websites (news, blogs), tags like div tag, span tag add noise.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- When extracting articles from news sites, you'll find lots of formatting tags. Models get confused by p tag, a href tag etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3ebc3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original Text:  <div>Hello <b>World</b>! NLP is <i>awesome</i>.</div> \n",
      "\n",
      "🟢 Remove HTML Tags with BS4:  Hello World! NLP is awesome. \n",
      "\n",
      "🔴 Remove HTML Tags with regex:  Hello World! NLP is awesome.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "\n",
    "# sample HTML text\n",
    "text = \"<div>Hello <b>World</b>! NLP is <i>awesome</i>.</div>\"\n",
    "\n",
    "# Using BeautifulSoup to remove HTML tags\n",
    "cleaned = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# Using regex to remove HTML tags\n",
    "re_pattern = \"<(?:\\\"[^\\\"]*\\\"['\\\"]*|'[^']*'['\\\"]*|[^'\\\">])+>\" # # Remove all HTML tags from a string\n",
    "cleaned_re = re.sub(re_pattern, '', text)\n",
    "# cleaned = cleaned.strip()\n",
    "\n",
    "print(\"🔵 Original Text: \", text, \"\\n\")\n",
    "print(\"🟢 Remove HTML Tags with BS4: \", cleaned, \"\\n\")\n",
    "print(\"🔴 Remove HTML Tags with regex: \", cleaned_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f8ff7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316925a",
   "metadata": {},
   "source": [
    "## 3️⃣ **Remove URLs**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- URLs usually don’t carry semantic meaning.\n",
    "\n",
    "- They are high variance strings (each one is unique) and hurt model generalization.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In a tweet like “Check this out 👉 https://xyz.com”, we care more about the sentiment or emotion, not the URL itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0ffdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original:  Visit our website at https://www.example.com/page.html or check out www.another-site.org. You can also find info at mydomain.net/info. \n",
      "\n",
      "🟢 Text Without URL:  Visit our website at  or check out  You can also find info at \n"
     ]
    }
   ],
   "source": [
    "# it will remove any type of URL from the text just apply on your text\n",
    "import regex as re\n",
    "\n",
    "# Example usage:\n",
    "text = \"Visit our website at https://www.example.com/page.html or check out www.another-site.org. You can also find info at mydomain.net/info.\"\n",
    "\n",
    "# Regex pattern to match various URL formats (http, https, www, without scheme)\n",
    "url_pattern = re.compile(\n",
    "    r'https?://[^\\s/$.?#].[^\\s]*'  # Matches http/https URLs\n",
    "    r'|www\\.[^\\s/$.?#].[^\\s]*'    # Matches www. URLs\n",
    "    r'|[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}(?:/[^\\s]*)?' # Matches domain-only URLs like example.com\n",
    ")\n",
    "\n",
    "# Remove URLs from the text\n",
    "text_without_urls= url_pattern.sub(r'', text)\n",
    "\n",
    "print(\"🔵 Original Text: \", text, \"\\n\")\n",
    "print(\"🟢 Text Without URL: \", text_without_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d70b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752634f5",
   "metadata": {},
   "source": [
    "## 4️⃣ **Remove Punctuation**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "-  Punctuation marks like `!`, `.`, `?` create unnecessary tokens.\n",
    "\n",
    "- Removing them helps reduce noise, especially in tasks like text classification or topic modeling.\n",
    "\n",
    "| Method                               | Removes ASCII | Removes Unicode | Customizable |\n",
    "| ------------------------------------ | ------------- | --------------- | ------------ |\n",
    "| `string.punctuation` + `translate()` | ✅ Yes         | ❌ No            | ❌ Limited    |\n",
    "| `re.sub(r'\\p{P}+', '', text)`       | ✅ Yes         | ✅ Yes           | ✅ Yes        |\n",
    "\n",
    "- string.punctuation only covers ASCII punctuation & returns: <div style=color:red;>!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~</div>\n",
    "\n",
    "- Unicode symbols like  `“`, `—`, and `…` are not included in string.punctuation, so we need to add them manually or using regex\n",
    "\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- For spam detection or document classification, punctuation doesn’t usually help (unless you’re analyzing writing style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b850b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original:  Hello!!! How are you??? I'm fine:) “Great”— yes… \n",
      "\n",
      "🟢 Text Cleaned From Punctuation Using string.punctuation:  Hello How are you Im fine Great yes\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation using string.punctuation\n",
    "import string\n",
    "\n",
    "text = \"Hello!!! How are you??? I'm fine:) “Great”— yes…\"\n",
    "no_punc = text.translate(str.maketrans('', '', string.punctuation + '“”—…'))\n",
    "\n",
    "print(\"🔵 Original Text: \", text, \"\\n\")\n",
    "print(\"🟢 Text Cleaned From Punctuation Using string.punctuation: \", no_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8e9421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original:  Hello!!! How are you??? I'm fine:) “Great”— yes… \n",
      "\n",
      "🟢 Text Cleaned From Punctuation Using Regex:  Hello How are you Im fine Great yes\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation using regex with Unicode support\n",
    "import regex as re\n",
    "\n",
    "text = \"Hello!!! How are you??? I'm fine:) “Great”— yes…\"\n",
    "no_punc = re.sub(r'\\p{P}+', '', text)\n",
    "\n",
    "print(\"🔵 Original Text: \", text, \"\\n\")\n",
    "print(\"🟢 Text Cleaned From Punctuation Using Regex: \", no_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e21be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5094c72",
   "metadata": {},
   "source": [
    "## 5️⃣ **Chat Word Treatment (e.g., GN → Good Night)**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Chat language is full of abbreviations: \"lol\", \"idk\", \"smh\".\n",
    "\n",
    "- They need to be normalized to standard English for models to understand.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In customer support or social media, replacing abbreviations like `\"brb\"` with `\"be right back\"` helps understand the message better.\n",
    "\n",
    "### 🗣️ **Slang Words**\n",
    "- The following two GitHub repositories provide comprehensive lists of slang words and their standard English equivalents.These resources are useful for expanding chat abbreviations (e.g., \"GN\" → \"Good Night\") during text preprocessing\n",
    "\n",
    "    - [Repo1](https://github.com/ipekdk/abbreviation-list-english)\n",
    "    - [Repo2](https://github.com/bodhwani/NLP-VIT-BOT/blob/master/slangs.csv)\n",
    "    - [Comman Slang Words](https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb4d81",
   "metadata": {},
   "source": [
    "#### Replace Slang words with the help of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "79551381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original Text:  BRB guys, lol this is funny. ttyl! \n",
      "\n",
      "🟢 Clean Slang Words Using Tokenization:  Be Right Back guys , Laughing Out Loud this is funny . Talk To You Later !\n"
     ]
    }
   ],
   "source": [
    "# Use spaCy for tokenization and keep punctuation marks\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"BRB guys, lol this is funny. ttyl!\"\n",
    "# Ensure 'nlp' is already loaded with spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)  # 'text' variable is already defined\n",
    "\n",
    "# Load slang dictionary from CSV\n",
    "chat_dict = pd.read_csv(\"slangs.csv\")  # Must have columns 'Abbr', 'Fullform'\n",
    "slang_map = dict(zip(chat_dict['Abbr'].str.lower(), chat_dict['Fullform']))\n",
    "\n",
    "# Replace slangs with full forms, preserving punctuation\n",
    "processed_tokens = []\n",
    "for token in doc:\n",
    "    token_text = token.text\n",
    "    lower_token = token_text.lower()\n",
    "    if lower_token in slang_map:\n",
    "        processed_tokens.extend(slang_map[lower_token].split())\n",
    "    else:\n",
    "        processed_tokens.append(token_text)\n",
    "\n",
    "preprocessed_text = \" \".join(processed_tokens)\n",
    "\n",
    "print(\"🔵 Original Text: \", text, \"\\n\")\n",
    "print(\"🟢 Clean Slang Words Using Tokenization: \", preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724952e5",
   "metadata": {},
   "source": [
    "#### Replace Slang word with the help of Split\n",
    "\n",
    "- you can see the difference `','` , `'.'` and `'!'` present in original text but not in preprocessed text using split but in the case of tokenization there is no missingness of `','` , `'.'` and `'!'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55030324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original:  BRB guys, lol this is funny. ttyl! \n",
      "\n",
      "🟢 Clean From Slang Words Using Regex:  Be Right Back guys Laughing Out Loud this is funny Talk To You Later\n"
     ]
    }
   ],
   "source": [
    "# just apply to your text this comprehenisve list of slang abbreviations\n",
    "# Make sure to have a CSV file named 'slangs.csv' with columns 'Abbr\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re  # Use the 'regex' module for Unicode property escapes\n",
    "\n",
    "# Load slang dictionary from CSV\n",
    "chat_dict = pd.read_csv(\"slangs.csv\")  # Must have columns 'Abbr', 'Fullform'\n",
    "slang_map = dict(zip(chat_dict['Abbr'].str.lower(), chat_dict['Fullform']))\n",
    "\n",
    "# Sample chat-like text\n",
    "text = \"BRB guys, lol this is funny. ttyl!\"\n",
    "\n",
    "# Remove punctuation using regex for matching text\n",
    "text_clean = re.sub(r'\\p{P}+', '', text)\n",
    "\n",
    "# Tokenize by whitespace\n",
    "words = text_clean.split()\n",
    "\n",
    "# Replace slangs\n",
    "chat_fixed = []\n",
    "for word in words:\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in slang_map:\n",
    "        chat_fixed.append(slang_map[word_lower])\n",
    "    else:\n",
    "        chat_fixed.append(word)\n",
    "\n",
    "# Join the cleaned text\n",
    "cleaned_text = \" \".join(chat_fixed) # The join() method takes all items in an iterable and joins them into one string. A string must be specified as the separator\n",
    "\n",
    "# Output\n",
    "print(\"🔵 Original Text: \", text, \"\\n\")\n",
    "print(\"🟢 Clean From Slang Words Using Regex: \", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c83a6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af2dbc",
   "metadata": {},
   "source": [
    "## 6️⃣ **Spelling Correction**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Typos are common in user input (especially social media).\n",
    "\n",
    "- Words like “beleive” won’t match “believe” in dictionaries or embeddings.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- Spell correction boosts chatbot understanding and auto-correction in search bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: I realy love natral language prosesing. \n",
      "\n",
      "🟢 Preprocessed (autocorrect):  I really love natural language crossing. \n",
      "\n",
      "🔴 Preprocessed (TextBlob):  I really love natural language pressing. \n",
      "\n",
      "🟡 Preprocessed (pyspellchecker):  I really love natural language prosesing.\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "text = \"I realy love natral language prosesing.\"\n",
    "\n",
    "# Autocorrect\n",
    "corrected_autocorrect = spell(text)\n",
    "\n",
    "# TextBlob\n",
    "corrected_textblob = str(TextBlob(text).correct())\n",
    "\n",
    "# SpellChecker\n",
    "spellchecker = SpellChecker()\n",
    "# Tokenize the text for word-level correction\n",
    "tokens = text.split()\n",
    "corrected_pyspellchecker = ' '.join([spellchecker.correction(word) or word for word in tokens])\n",
    "\n",
    "\n",
    "print(\"🔵 Original Text:\", text, \"\\n\")\n",
    "print(\"🟢 Preprocessed (autocorrect): \", corrected_autocorrect, \"\\n\")\n",
    "print(\"🔴 Preprocessed (TextBlob): \", corrected_textblob, \"\\n\")\n",
    "print(\"🟡 Preprocessed (pyspellchecker): \", corrected_pyspellchecker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e06efd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f78e50",
   "metadata": {},
   "source": [
    "## 7️⃣ **Remove Stop Words**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Stopwords like \"the\", \"is\", \"at\" are function words, not content words.\n",
    "\n",
    "- Removing them improves focus on important tokens.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In a movie review, “The movie was absolutely fantastic”, word “fantastic” carries the sentiment, not “the”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63cfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original:  This is a sample sentence with some stop words \n",
      "\n",
      "🟢 Text After Removing Stop Words:  sample sentence stop words \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "import spacy\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "text = \"This is a sample sentence with some stop words\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Remove stop words\n",
    "# This line creates a list of tokens that are not stop words and joins them into a single string\n",
    "# is_stop is a boolean attribute of the token object that indicates whether the token is a stop word\n",
    "filtered_tokens = \" \".join([token.text for token in doc if not token.is_stop]) \n",
    "\n",
    "# Print the text excluding stop words\n",
    "print(\"🔵 Original Text: \", text, \"\\n\")\n",
    "print(\"🟢 Text After Removing Stop Words: \", filtered_tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc3974",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680edd1",
   "metadata": {},
   "source": [
    "## 8️⃣ **Handling Emojis**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Emojis are sentiment-rich tokens (😊, 😢).\n",
    "\n",
    "- You can either remove them, convert to words, or use them as features.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In social media sentiment analysis, 😠 and ❤️ change the tone completely and must be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e03834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: Good job!👍I’m so happy😊\n",
      "\n",
      "🟢 Text With Handled Emojis: Good job! thumbs up I’m so happy smiling face with smiling eyes \n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import regex as re\n",
    "\n",
    "text = \"Good job!👍I’m so happy😊\"\n",
    "emoji_text = emoji.demojize(text)\n",
    "\n",
    "# Remove ':' and '_' characters\n",
    "emoji_text_cleaned = emoji_text.replace(':', ' ').replace('_', ' ')\n",
    "\n",
    "print(f\"🔵 Original Text: {text}\\n\")\n",
    "print(f\"🟢 Text With Handled Emojis: {emoji_text_cleaned}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee66ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f43ef",
   "metadata": {},
   "source": [
    "## 9️⃣ **Tokenization**\n",
    "\n",
    "### ⚠️ **Very Important Step**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "Machine learning models in NLP typically require numerical input. Tokenization converts text into a format that can be converted into numerical representations (like word embeddings) that models can understand.\n",
    "\n",
    "#### **Example:**\n",
    "Let's say we have the sentence: `The quick brown fox jumps over the lazy dog.`\n",
    "Word Tokenization:\n",
    "> [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In the search engine, \"best Italian restaurants near me\" is tokenized into the words: [\"best\", \"Italian\", \"restaurants\", \"near\", \"me\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a10828",
   "metadata": {},
   "source": [
    "#### Tokenization with `spacy.blank(\"en\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2221d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original:  Hello, world! This is a test sentence. Let's see how it works. \n",
      "\n",
      "🟢 After Tokenization Using spacy.blank('en'):  ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', '.', 'Let', \"'s\", 'see', 'how', 'it', 'works', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Create a blank English NLP object (only tokenizer) No lemmatization, NER, POS & Tagging\n",
    "nlp_blank = spacy.blank(\"en\")\n",
    "\n",
    "# Input text\n",
    "text = \"Hello, world! This is a test sentence. Let's see how it works.\"\n",
    "\n",
    "# Tokenize using blank pipeline\n",
    "doc_blank = nlp_blank(text)\n",
    "\n",
    "# Print each token\n",
    "token_list = [token.text for token in doc_blank]\n",
    "\n",
    "print(\"🔵 Original Text: \", text, \"\\n\")\n",
    "print(f\"🟢 After Tokenization Using spacy.blank('en'): \", token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f7c66",
   "metadata": {},
   "source": [
    "#### Tokenization with `spacy.load(\"en_core_web_sm\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65bf405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original:  Hello, world! This is a test sentence. Let's see how it works. \n",
      "\n",
      "🟢 After Tokenization Using English Model:  ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', '.', 'Let', \"'s\", 'see', 'how', 'it', 'works', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load pretrained small English model (includes tokenizer, POS, NER, etc.)\n",
    "nlp_full = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input text\n",
    "text = \"Hello, world! This is a test sentence. Let's see how it works.\"\n",
    "\n",
    "# Process text\n",
    "doc_full = nlp_full(text)\n",
    "\n",
    "# Print each token\n",
    "token_list = [token.text for token in doc_full]\n",
    "\n",
    "print(\"🔵 Original Text: \", text, \"\\n\")\n",
    "print(\"🟢 After Tokenization Using English Model: \", token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b63df1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e6015",
   "metadata": {},
   "source": [
    "## 🔟 **Stemming**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Stemming reduces words to a base/root form. It’s fast and works well in information retrieval systems.\n",
    "\n",
    "- May produce non-words (e.g., “studies” → “studi”).\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- Used in search engines (e.g., “searching”, “searched”, “searches” → “search”) to show relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed7ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original:  ['Organizing', 'events', 'helps', 'communities', 'grow', 'and', 'stay', 'connected', '.'] \n",
      "\n",
      "🟢 After Stemming Using PorterStemmer:  ['organ', 'event', 'help', 'commun', 'grow', 'and', 'stay', 'connect', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Load spaCy (for tokenization)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Three well-known stemming algorithms are the Porter stemmer, Snowball stemmer, and Lancaster stemmer\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Input text\n",
    "text = \"Organizing events helps communities grow and stay connected.\"\n",
    "\n",
    "# Tokenize using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Apply stemming using NLTK\n",
    "stemmed_tokens = [stemmer.stem(token.text) for token in doc]\n",
    "\n",
    "# Output\n",
    "print(\"🔵 Original Text: \", [token.text for token in doc], \"\\n\")\n",
    "print(\"🟢 After Stemming Using PorterStemmer: \", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67c16e",
   "metadata": {},
   "source": [
    "#### **The 3 Main NLTK Stemmers + Lemmatization**\n",
    "\n",
    "| Technique            | Aggressiveness   | Language Support                 | Speed                     | Output Quality               | Notes                                                               |\n",
    "| -------------------- | ---------------- | -------------------------------- | ------------------------- | ---------------------------- | ------------------------------------------------------------------- |\n",
    "| **PorterStemmer**    | ✅ Moderate       | English only                     | ✅ Fast                    | ✅ Good, widely used          | Most commonly used, balanced stemming algorithm                     |\n",
    "| **LancasterStemmer** | 🔥 Very high     | English only                     | ✅ Fast                    | ❌ Over-stems words           | Very aggressive → `maximum` → `maxim`                       |\n",
    "| **SnowballStemmer**  | 🟢 Balanced      | ✅ Multiple langs                 | ✅ Fast                    | ✅ Clean, improved            | Also called “Porter2” — better, more consistent than Porter         |\n",
    "| **Lemmatization**    | ❌ Not aggressive | ✅ Multiple langs (WordNet/spaCy) | ⚠️ Slower (context-aware) | ✅ Real words & context-aware | Uses grammar + vocabulary, returns meaningful dictionary base forms |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd86751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original:  The organization organized organized events efficiently. \n",
      "\n",
      "🟢 PorterStemmer:  ['the', 'organ', 'organ', 'organ', 'event', 'effici', '.'] \n",
      "\n",
      "🔴 LancasterStemmer:  ['the', 'org', 'org', 'org', 'ev', 'efficy', '.'] \n",
      "\n",
      "🟡 SnowballStemmer:  ['the', 'organ', 'organ', 'organ', 'event', 'effici', '.'] \n",
      "\n",
      "🟣 Lemmatization:  ['the', 'organization', 'organize', 'organize', 'event', 'efficiently', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"The organization organized organized events efficiently.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = nlp(text)\n",
    "\n",
    "# Initialize stemmers\n",
    "porter = PorterStemmer() # Balanced stemmer\n",
    "lancaster = LancasterStemmer() # aggressive stemmer\n",
    "snowball = SnowballStemmer(\"english\") # Also Called Porter2\n",
    "\n",
    "# Apply each stemmer\n",
    "porter_result = [porter.stem(word.text) for word in tokens]\n",
    "lancaster_result = [lancaster.stem(word.text) for word in tokens]\n",
    "snowball_result = [snowball.stem(word.text) for word in tokens]\n",
    "lemmized_result = [word.lemma_ for word in tokens]  # Lemmatization using spaCy\n",
    "\n",
    "print(\"🔵 Original Text: \", tokens, \"\\n\")\n",
    "print(\"🟢 PorterStemmer: \", porter_result, \"\\n\")\n",
    "print(\"🔴 LancasterStemmer: \", lancaster_result, \"\\n\")\n",
    "print(\"🟡 SnowballStemmer: \", snowball_result, \"\\n\")\n",
    "print(\"🟣 Lemmatization: \", lemmized_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8fc55b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c15c0",
   "metadata": {},
   "source": [
    "## 🔢 **Lemmatization**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Like stemming, but linguistically accurate.\n",
    "\n",
    "- Uses vocabulary and grammar rules to return the correct base word.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- For grammar-sensitive tasks like question answering or summarization, “was” should be lemmatized to “be”, not “wa”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2227650",
   "metadata": {},
   "source": [
    "#### **Comparison: Lemmatization in spaCy vs NLTK**\n",
    "| Feature           | **spaCy**                         | **NLTK (WordNetLemmatizer)**         |\n",
    "| ----------------- | --------------------------------- | ------------------------------------ |\n",
    "| Accuracy          | ✅ High (uses POS + context)       | ⚠️ Medium (needs manual POS mapping) |\n",
    "| Language model    | `en_core_web_sm` or larger models | WordNet dictionary                   |\n",
    "| POS Tag Awareness | ✅ Automatic                       | ⚠️ Must provide POS manually         |\n",
    "| Output Quality    | ✅ Real, accurate base forms       | ✅ Good, but context-blind sometimes  |\n",
    "| Ease of Use       | ✅ Very easy                       | ⚠️ Slightly complex (POS conversion) |\n",
    "| Performance       | ✅ Fast and optimized              | ✅ Fast but rule-based                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ea81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original:  The organization organized organized events efficiently. \n",
      "\n",
      "🟢 After Lemmatization:  the child be run and well organize event every week . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"The children were running and better organized events every week.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Lemmatize with POS awareness\n",
    "lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "# Output\n",
    "print(\"🔵 Original Text: \", tokens, \"\\n\")\n",
    "print(\"🟢 After Lemmatization: \", \" \".join(lemmatized_tokens), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0caad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Lemmas: ['The', 'child', 'be', 'run', 'and', 'good', 'organize', 'event', 'every', 'week', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# POS tag conversion\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "text = \"The children were running and better organized events every week.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "result = [tokens.text for tokens in doc]\n",
    "tagged = pos_tag(result)\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n",
    "print(\"NLTK Lemmas:\", lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2105d408",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d646831",
   "metadata": {},
   "source": [
    "## ✅ Summary Table\n",
    "\n",
    "| Step                | Purpose                                                                 | Use Case Examples                                     |\n",
    "|---------------------|-------------------------------------------------------------------------|--------------------------------------------------------|\n",
    "| Lowercasing          | Normalize text casing                                                   | Text classification, search                           |\n",
    "| Remove HTML Tags     | remove unwanted HTML tags                                                | Web scraping, email cleaning                          |\n",
    "| Remove URLs          | Remove unnecessary web links                                             | Social media, forums                                  |\n",
    "| Remove Punctuation   | Clean punctuation noise                                                  | Preprocessing for BoW/TF-IDF                          |\n",
    "| Chat Word Treatment  | Convert slang to standard English like GD to Good Night                  | Chatbots, social media analysis                       |\n",
    "| Spelling Correction  | Fix typos for vocabulary consistency                                     | User reviews, text input                              |\n",
    "| Remove Stop Words    | Focus on meaningful words                                                 | Summarization, topic modeling                         |\n",
    "| Handle Emojis        | Preserve or convert emojis based on task                                | Sentiment analysis                                    |\n",
    "| Tokenization         | Break down text into tokens                                              | All NLP tasks                                         |\n",
    "| Stemming             | Reduce word forms to base/root                                           | Search engines, topic modeling                        |\n",
    "| Lemmatization        | Get accurate root word using grammar                                     | Parsing, QA systems, deep learning models             |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
