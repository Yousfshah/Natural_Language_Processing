{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90808d43",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .highlight-text {\n",
    "        font-family: 'Arial', sans-serif;\n",
    "        font-size: 3em; /* Increased for heading prominence */\n",
    "        font-weight: bold;\n",
    "        background: linear-gradient(45deg, #3498db, #8e44ad);\n",
    "        -webkit-background-clip: text;\n",
    "        -webkit-text-fill-color: transparent;\n",
    "        text-shadow: 3px 3px 6px rgba(0, 0, 0, 0.3); /* Enhanced text shadow */\n",
    "        display: block;\n",
    "        text-align: center; /* Center the text */\n",
    "        padding: 15px 20px; /* Increased padding for heading feel */\n",
    "        border-radius: 10px;\n",
    "        background-color: #153160ff; /* Retained background color */\n",
    "        box-shadow: 0 4px 12px 4px rgba(0, 0, 0); /* Added box shadow for depth */\n",
    "        margin: 20px auto; /* Centered with auto margins */\n",
    "        max-width: 80%; /* Prevents overly wide heading */\n",
    "        transition: transform 0.3s ease, box-shadow 0.3s ease; /* Smooth hover effects */\n",
    "    }\n",
    "\n",
    "    .highlight-text:hover {\n",
    "        transform: scale(1.05); /* Subtle zoom on hover */\n",
    "        box-shadow: 0 6px 16px rgba(0, 0, 0, 0.3); /* Enhanced shadow on hover */\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<span class=\"highlight-text\">Text Preprocessing</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3a973",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 650px; margin: 40px auto; padding: 25px; border-radius: 10px; text-align: center; background: linear-gradient(135deg, #000000ff, #000000ff); border: 2px solid #ffffff; box-shadow: 0 6px 20px rgba(0, 0, 0, 0.15);\" onmouseover=\"this.style.transform='translateY(-3px)'; this.style.boxShadow='0 8px 25px rgba(0, 0, 0, 0.2)'\" onmouseout=\"this.style.transform='translateY(0)'; this.style.boxShadow='0 6px 20px rgba(0, 0, 0, 0.15)'\">\n",
    "  <h3 style=\"font-size: 24px; font-weight: 700; color: #212121; margin-bottom: 20px; font-family: 'Lato', Arial, sans-serif; letter-spacing: 1px;\">\n",
    "    <strong style = color:white>Author Name:</strong>\n",
    "    <a href=\"https://www.linkedin.com/in/yousuf-shah-7ba9492b4/\" target=\"_blank\" style=\"color: #daff06ff; text-decoration: none; font-weight: bold;\">Yousuf Shah</a>\n",
    "  </h3>\n",
    "  <table style=\"margin: 0 auto; border-spacing: 20px;\">\n",
    "    <tr>\n",
    "      <td>\n",
    "        <a href=\"https://www.linkedin.com/in/yousuf-shah-7ba9492b4/\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/LinkedIn-Profile-blue?style=for-the-badge&logo=linkedin\" alt=\"LinkedIn\" style=\"height: 55px; border-radius: 8px; background: linear-gradient(45deg, #000000, #000000); padding: 5px;\"/>\n",
    "        </a>\n",
    "      </td>\n",
    "      <td>\n",
    "        <a href=\"https://yousfshah.github.io/Portfolio_Website/\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/Portfolio-Website-blue?style=for-the-badge&logo=link\" alt=\"Portfolio Website\" style=\"height: 55px; border-radius: 8px; background: linear-gradient(45deg, #000000, #000000); padding: 5px;\"/>\n",
    "        </a>\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\n",
    "        <a href=\"https://github.com/Yousfshah\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/GitHub-Profile-green?style=for-the-badge&logo=github\" alt=\"GitHub\" style=\"height: 55px; border-radius: 8px; background: linear-gradient(45deg, #000000, #000000); padding: 5px;\"/>\n",
    "        </a>\n",
    "      </td>\n",
    "      <td>\n",
    "        <a href=\"https://www.kaggle.com/yousufshah\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/Kaggle-Profile-orange?style=for-the-badge&logo=kaggle\" alt=\"Kaggle\" style=\"height: 55px; border-radius: 8px; background: linear-gradient(45deg, #000000, #000000); padding: 5px;\"/>\n",
    "        </a>\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        <a href=\"https://yousfshah.github.io/Blogging/\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/Blog-Posts-purple?style=for-the-badge&logo=blogger\" alt=\"Blog\" style=\"height: 55px; border-radius: 8px; background: linear-gradient(45deg, #000000, #000000); padding: 5px;\"/>\n",
    "        </a>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48d9b3d",
   "metadata": {},
   "source": [
    "<style>\n",
    "  /* General Reset */\n",
    "  * {\n",
    "    margin: 0;\n",
    "    padding: 0;\n",
    "    box-sizing: border-box;\n",
    "  }\n",
    "\n",
    "  /* Contact Info Section */\n",
    "  .contact-info {\n",
    "    max-width: 700px; /* Slightly wider for better spacing */\n",
    "    margin: 30px auto;\n",
    "    padding: 25px;\n",
    "    border-radius: 20px;\n",
    "    text-align: center;\n",
    "    background: linear-gradient(135deg, #ffffffff, #000d0cff); /* Dark gradient for modern look */\n",
    "    border: 3px solid #000000ff; /* Vibrant border */\n",
    "    box-shadow: 0 8px 25px rgba(0, 0, 0, 0.5), inset 0 0 10px rgba(255, 255, 255, 0.2); /* 3D effect */\n",
    "    transition: transform 0.3s ease, box-shadow 0.3s ease;\n",
    "  }\n",
    "\n",
    "  .contact-info:hover {\n",
    "    transform: translateY(-5px); /* Lift effect on hover */\n",
    "    box-shadow: 0 12px 30px rgba(119, 0, 0, 0.6), inset 0 0 15px rgba(122, 0, 86, 0.3);\n",
    "  }\n",
    "\n",
    "  /* Section Titles */\n",
    "  .section-title {\n",
    "    font-size: 24px;\n",
    "    font-weight: bold;\n",
    "    color: #0a0101ff; /* White for contrast */\n",
    "    margin-bottom: 20px;\n",
    "    text-shadow: 0 0 8px rgba(2, 62, 19, 0.7); /* Glowing effect */\n",
    "    font-family: 'Roboto', Arial, sans-serif;\n",
    "  }\n",
    "\n",
    "  .section-title a {\n",
    "    color: #02303cff; /* Vibrant cyan */\n",
    "    text-decoration: none;\n",
    "    transition: color 0.3s ease, text-shadow 0.3s ease;\n",
    "  }\n",
    "\n",
    "  .section-title a:hover {\n",
    "    color: #340000ff; /* Coral on hover */\n",
    "    text-shadow: 0 0 12px rgba(84, 0, 76, 0.8);\n",
    "  }\n",
    "\n",
    "  /* Table for Social Links */\n",
    "  .contact-info table {\n",
    "    margin: 0 auto;\n",
    "    border-spacing: 20px; /* Increased for better separation */\n",
    "  }\n",
    "\n",
    "  /* Social Media Badges */\n",
    "  .contact-info img {\n",
    "    height: 60px; /* Slightly larger badges */\n",
    "    border-radius: 12px;\n",
    "    transition: all 0.3s ease;\n",
    "    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3); /* Initial shadow */\n",
    "    background: linear-gradient(45deg, #000000ff, #4d4d4dff); /* Card-like background */\n",
    "    padding: 5px; /* Subtle padding for card effect */\n",
    "  }\n",
    "\n",
    "  .contact-info img:hover {\n",
    "    transform: scale(1.2) rotate(5deg); /* Scale and slight rotation */\n",
    "    box-shadow: 0 8px 20px rgba(0, 0, 0, 0.5), 0 0 15px rgba(176, 192, 0, 0.7); /* Glowing shadow */\n",
    "    background: linear-gradient(45deg, #2b0353ff, #610012ff); /* Gradient on hover */\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<div class=\"contact-info\">\n",
    "  <h3 class=\"section-title\">\n",
    "    <strong>Author Name:</strong>\n",
    "    <a href=\"https://www.linkedin.com/in/yousuf-shah-7ba9492b4/\" target=\"_blank\">Yousuf Shah</a>\n",
    "  </h3>\n",
    "  <table>\n",
    "    <tr>\n",
    "      <td>\n",
    "        <a href=\"https://www.linkedin.com/in/yousuf-shah-7ba9492b4/\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/LinkedIn-Profile-blue?style=for-the-badge&logo=linkedin\" alt=\"LinkedIn\" />\n",
    "        </a>\n",
    "      </td>\n",
    "      <td>\n",
    "        <a href=\"https://yousfshah.github.io/Portfolio_Website/\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/Portfolio_Website-Website-blue?style=for-the-badge&logo=link\" alt=\"Portfolio Website\" />\n",
    "        </a>\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\n",
    "        <a href=\"https://github.com/Yousfshah\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/GitHub-Profile-green?style=for-the-badge&logo=github\" alt=\"GitHub\" />\n",
    "        </a>\n",
    "      </td>\n",
    "      <td>\n",
    "        <a href=\"https://www.kaggle.com/yousufshah\" target=\"_blank\">\n",
    "          <img src=\"https://img.shields.io/badge/Kaggle-Profile-orange?style=for-the-badge&logo=kaggle\" alt=\"Kaggle\" />\n",
    "        </a>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910a6b4",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 800px; margin: 2.5rem auto; padding: 2rem 2.5rem; background: linear-gradient(135deg, #8d8d8dff, #9c9b9bff); box-shadow: 0 0.75rem 1.5rem rgba(0, 0, 0, 0.08); font-family: 'Segoe UI', 'Lato', sans-serif; color: #111; border-left: 6px solid #0055ff; border-radius: 0.5rem;\">\n",
    "\n",
    "  <h3 style=\"font-size: 1.8rem; font-weight: 700; margin-bottom: 1.2rem; color: #000000ff;\">\n",
    "    üìå Why You Should Practice This Notebook\n",
    "  </h3>\n",
    "\n",
    "  <p style=\"font-size: 1.05rem; line-height: 1.7; margin-bottom: 1rem; color:#000; font-weight: bold;\">\n",
    "    If you're serious about mastering NLP, this notebook gives you a hands-on, end-to-end experience with real-world text preprocessing ‚Äî the foundation of every NLP project.\n",
    "    You'll learn to clean, normalize, and prepare raw text using both industry-standard tools (<strong>spaCy</strong> + <strong>NLTK</strong>) and best practices followed in real ML pipelines.\n",
    "  </p>\n",
    "\n",
    "  <ul style=\"font-size: 1.05rem; line-height: 1.7; padding-left: 1.2rem; margin-bottom: 1rem; color:#000; font-weight: bold;\">\n",
    "    <li>‚úÖ Practical code examples</li>\n",
    "    <li>‚úÖ Reusable functions</li>\n",
    "    <li>‚úÖ Cleaned input ready for ML, BERT, or vectorization</li>\n",
    "  </ul>\n",
    "\n",
    "  <p style=\"font-size: 1.05rem; line-height: 1.7; font-weight: 600; color: #ab1700ff;\">\n",
    "    Good models start with great preprocessing.<br>\n",
    "  </p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f888d2",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ **Lowercasing**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Words like \"Good\", \"GOOD\", and \"good\" are semantically identical but are treated as separate tokens if not lowercased.\n",
    "\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In a product review system, \"Excellent\" and \"excellent\" should be considered the same sentiment word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6cbb363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  Good Morning EVERYONE! Let's Start Our NLP Journey. \n",
      "\n",
      "üü¢ Lower Case Text:  good morning everyone! let's start our nlp journey.\n"
     ]
    }
   ],
   "source": [
    "text = \"Good Morning EVERYONE! Let's Start Our NLP Journey.\"\n",
    "lowercased = text.lower()\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Lower Case Text: \", lowercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a091a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f3aa0",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ **Remove HTML Tags**\n",
    "\n",
    "### üîç **Why:**\n",
    "- HTML is markup used for layout, not meaning.\n",
    "\n",
    "- If you're scraping websites (news, blogs), tags like div tag, span tag add noise.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- When extracting articles from news sites, you'll find lots of formatting tags. Models get confused by p tag, a href tag etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3ebc3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  <div>Hello <b>World</b>! NLP is <i>awesome</i>.</div> \n",
      "\n",
      "üü¢ Remove HTML Tags with BS4:  Hello World! NLP is awesome. \n",
      "\n",
      "üî¥ Remove HTML Tags with regex:  Hello World! NLP is awesome.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "\n",
    "# sample HTML text\n",
    "text = \"<div>Hello <b>World</b>! NLP is <i>awesome</i>.</div>\"\n",
    "\n",
    "# Using BeautifulSoup to remove HTML tags\n",
    "cleaned = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# Using regex to remove HTML tags\n",
    "re_pattern = \"<(?:\\\"[^\\\"]*\\\"['\\\"]*|'[^']*'['\\\"]*|[^'\\\">])+>\" # # Remove all HTML tags from a string\n",
    "cleaned_re = re.sub(re_pattern, '', text)\n",
    "# cleaned = cleaned.strip()\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Remove HTML Tags with BS4: \", cleaned, \"\\n\")\n",
    "print(\"üî¥ Remove HTML Tags with regex: \", cleaned_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f8ff7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316925a",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ **Remove URLs**\n",
    "\n",
    "### üîç **Why:**\n",
    "- URLs usually don‚Äôt carry semantic meaning.\n",
    "\n",
    "- They are high variance strings (each one is unique) and hurt model generalization.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In a tweet like ‚ÄúCheck this out üëâ https://xyz.com‚Äù, we care more about the sentiment or emotion, not the URL itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0ffdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original:  Visit our website at https://www.example.com/page.html or check out www.another-site.org. You can also find info at mydomain.net/info. \n",
      "\n",
      "üü¢ Text Without URL:  Visit our website at  or check out  You can also find info at \n"
     ]
    }
   ],
   "source": [
    "# it will remove any type of URL from the text just apply on your text\n",
    "import regex as re\n",
    "\n",
    "# Example usage:\n",
    "text = \"Visit our website at https://www.example.com/page.html or check out www.another-site.org. You can also find info at mydomain.net/info.\"\n",
    "\n",
    "# Regex pattern to match various URL formats (http, https, www, without scheme)\n",
    "url_pattern = re.compile(\n",
    "    r'https?://[^\\s/$.?#].[^\\s]*'  # Matches http/https URLs\n",
    "    r'|www\\.[^\\s/$.?#].[^\\s]*'    # Matches www. URLs\n",
    "    r'|[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}(?:/[^\\s]*)?' # Matches domain-only URLs like example.com\n",
    ")\n",
    "\n",
    "# Remove URLs from the text\n",
    "text_without_urls= url_pattern.sub(r'', text)\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Text Without URL: \", text_without_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d70b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752634f5",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ **Remove Punctuation**\n",
    "\n",
    "### üîç **Why:**\n",
    "-  Punctuation marks like `!`, `.`, `?` create unnecessary tokens.\n",
    "\n",
    "- Removing them helps reduce noise, especially in tasks like text classification or topic modeling.\n",
    "\n",
    "| Method                               | Removes ASCII | Removes Unicode | Customizable |\n",
    "| ------------------------------------ | ------------- | --------------- | ------------ |\n",
    "| `string.punctuation` + `translate()` | ‚úÖ Yes         | ‚ùå No            | ‚ùå Limited    |\n",
    "| `re.sub(r'\\p{P}+', '', text)`       | ‚úÖ Yes         | ‚úÖ Yes           | ‚úÖ Yes        |\n",
    "\n",
    "- string.punctuation only covers ASCII punctuation & returns: <div style=color:red;>!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~</div>\n",
    "\n",
    "- Unicode symbols like  `‚Äú`, `‚Äî`, and `‚Ä¶` are not included in string.punctuation, so we need to add them manually or using regex\n",
    "\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- For spam detection or document classification, punctuation doesn‚Äôt usually help (unless you‚Äôre analyzing writing style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b850b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original:  Hello!!! How are you??? I'm fine:) ‚ÄúGreat‚Äù‚Äî yes‚Ä¶ \n",
      "\n",
      "üü¢ Text Cleaned From Punctuation Using string.punctuation:  Hello How are you Im fine Great yes\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation using string.punctuation\n",
    "import string\n",
    "\n",
    "text = \"Hello!!! How are you??? I'm fine:) ‚ÄúGreat‚Äù‚Äî yes‚Ä¶\"\n",
    "no_punc = text.translate(str.maketrans('', '', string.punctuation + '‚Äú‚Äù‚Äî‚Ä¶'))\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Text Cleaned From Punctuation Using string.punctuation: \", no_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8e9421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original:  Hello!!! How are you??? I'm fine:) ‚ÄúGreat‚Äù‚Äî yes‚Ä¶ \n",
      "\n",
      "üü¢ Text Cleaned From Punctuation Using Regex:  Hello How are you Im fine Great yes\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation using regex with Unicode support\n",
    "import regex as re\n",
    "\n",
    "text = \"Hello!!! How are you??? I'm fine:) ‚ÄúGreat‚Äù‚Äî yes‚Ä¶\"\n",
    "no_punc = re.sub(r'\\p{P}+', '', text)\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Text Cleaned From Punctuation Using Regex: \", no_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e21be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5094c72",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ **Chat Word Treatment (e.g., GN ‚Üí Good Night)**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Chat language is full of abbreviations: \"lol\", \"idk\", \"smh\".\n",
    "\n",
    "- They need to be normalized to standard English for models to understand.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In customer support or social media, replacing abbreviations like `\"brb\"` with `\"be right back\"` helps understand the message better.\n",
    "\n",
    "### üó£Ô∏è **Slang Words**\n",
    "- The following two GitHub repositories provide comprehensive lists of slang words and their standard English equivalents.These resources are useful for expanding chat abbreviations (e.g., \"GN\" ‚Üí \"Good Night\") during text preprocessing\n",
    "\n",
    "    - [Repo1](https://github.com/ipekdk/abbreviation-list-english)\n",
    "    - [Repo2](https://github.com/bodhwani/NLP-VIT-BOT/blob/master/slangs.csv)\n",
    "    - [Comman Slang Words](https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb4d81",
   "metadata": {},
   "source": [
    "#### Replace Slang words with the help of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "79551381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:  BRB guys, lol this is funny. ttyl! \n",
      "\n",
      "üü¢ Clean Slang Words Using Tokenization:  Be Right Back guys , Laughing Out Loud this is funny . Talk To You Later !\n"
     ]
    }
   ],
   "source": [
    "# Use spaCy for tokenization and keep punctuation marks\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"BRB guys, lol this is funny. ttyl!\"\n",
    "# Ensure 'nlp' is already loaded with spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)  # 'text' variable is already defined\n",
    "\n",
    "# Load slang dictionary from CSV\n",
    "chat_dict = pd.read_csv(\"slangs.csv\")  # Must have columns 'Abbr', 'Fullform'\n",
    "slang_map = dict(zip(chat_dict['Abbr'].str.lower(), chat_dict['Fullform']))\n",
    "\n",
    "# Replace slangs with full forms, preserving punctuation\n",
    "processed_tokens = []\n",
    "for token in doc:\n",
    "    token_text = token.text\n",
    "    lower_token = token_text.lower()\n",
    "    if lower_token in slang_map:\n",
    "        processed_tokens.extend(slang_map[lower_token].split())\n",
    "    else:\n",
    "        processed_tokens.append(token_text)\n",
    "\n",
    "preprocessed_text = \" \".join(processed_tokens)\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Clean Slang Words Using Tokenization: \", preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724952e5",
   "metadata": {},
   "source": [
    "#### Replace Slang word with the help of Split\n",
    "\n",
    "- you can see the difference `','` , `'.'` and `'!'` present in original text but not in preprocessed text using split but in the case of tokenization there is no missingness of `','` , `'.'` and `'!'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55030324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original:  BRB guys, lol this is funny. ttyl! \n",
      "\n",
      "üü¢ Clean From Slang Words Using Regex:  Be Right Back guys Laughing Out Loud this is funny Talk To You Later\n"
     ]
    }
   ],
   "source": [
    "# just apply to your text this comprehenisve list of slang abbreviations\n",
    "# Make sure to have a CSV file named 'slangs.csv' with columns 'Abbr\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re  # Use the 'regex' module for Unicode property escapes\n",
    "\n",
    "# Load slang dictionary from CSV\n",
    "chat_dict = pd.read_csv(\"slangs.csv\")  # Must have columns 'Abbr', 'Fullform'\n",
    "slang_map = dict(zip(chat_dict['Abbr'].str.lower(), chat_dict['Fullform']))\n",
    "\n",
    "# Sample chat-like text\n",
    "text = \"BRB guys, lol this is funny. ttyl!\"\n",
    "\n",
    "# Remove punctuation using regex for matching text\n",
    "text_clean = re.sub(r'\\p{P}+', '', text)\n",
    "\n",
    "# Tokenize by whitespace\n",
    "words = text_clean.split()\n",
    "\n",
    "# Replace slangs\n",
    "chat_fixed = []\n",
    "for word in words:\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in slang_map:\n",
    "        chat_fixed.append(slang_map[word_lower])\n",
    "    else:\n",
    "        chat_fixed.append(word)\n",
    "\n",
    "# Join the cleaned text\n",
    "cleaned_text = \" \".join(chat_fixed) # The join() method takes all items in an iterable and joins them into one string. A string must be specified as the separator\n",
    "\n",
    "# Output\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Clean From Slang Words Using Regex: \", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c83a6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af2dbc",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ **Spelling Correction**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Typos are common in user input (especially social media).\n",
    "\n",
    "- Words like ‚Äúbeleive‚Äù won‚Äôt match ‚Äúbelieve‚Äù in dictionaries or embeddings.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- Spell correction boosts chatbot understanding and auto-correction in search bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: I realy love natral language prosesing. \n",
      "\n",
      "üü¢ Preprocessed (autocorrect):  I really love natural language crossing. \n",
      "\n",
      "üî¥ Preprocessed (TextBlob):  I really love natural language pressing. \n",
      "\n",
      "üü° Preprocessed (pyspellchecker):  I really love natural language prosesing.\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "text = \"I realy love natral language prosesing.\"\n",
    "\n",
    "# Autocorrect\n",
    "corrected_autocorrect = spell(text)\n",
    "\n",
    "# TextBlob\n",
    "corrected_textblob = str(TextBlob(text).correct())\n",
    "\n",
    "# SpellChecker\n",
    "spellchecker = SpellChecker()\n",
    "# Tokenize the text for word-level correction\n",
    "tokens = text.split()\n",
    "corrected_pyspellchecker = ' '.join([spellchecker.correction(word) or word for word in tokens])\n",
    "\n",
    "\n",
    "print(\"üîµ Original Text:\", text, \"\\n\")\n",
    "print(\"üü¢ Preprocessed (autocorrect): \", corrected_autocorrect, \"\\n\")\n",
    "print(\"üî¥ Preprocessed (TextBlob): \", corrected_textblob, \"\\n\")\n",
    "print(\"üü° Preprocessed (pyspellchecker): \", corrected_pyspellchecker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e06efd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f78e50",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ **Remove Stop Words**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Stopwords like \"the\", \"is\", \"at\" are function words, not content words.\n",
    "\n",
    "- Removing them improves focus on important tokens.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In a movie review, ‚ÄúThe movie was absolutely fantastic‚Äù, word ‚Äúfantastic‚Äù carries the sentiment, not ‚Äúthe‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63cfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original:  This is a sample sentence with some stop words \n",
      "\n",
      "üü¢ Text After Removing Stop Words:  sample sentence stop words \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "import spacy\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "text = \"This is a sample sentence with some stop words\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Remove stop words\n",
    "# This line creates a list of tokens that are not stop words and joins them into a single string\n",
    "# is_stop is a boolean attribute of the token object that indicates whether the token is a stop word\n",
    "filtered_tokens = \" \".join([token.text for token in doc if not token.is_stop]) \n",
    "\n",
    "# Print the text excluding stop words\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ Text After Removing Stop Words: \", filtered_tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc3974",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680edd1",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ **Handling Emojis**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Emojis are sentiment-rich tokens (üòä, üò¢).\n",
    "\n",
    "- You can either remove them, convert to words, or use them as features.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In social media sentiment analysis, üò† and ‚ù§Ô∏è change the tone completely and must be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e03834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: Good job!üëçI‚Äôm so happyüòä\n",
      "\n",
      "üü¢ Text With Handled Emojis: Good job! thumbs up I‚Äôm so happy smiling face with smiling eyes \n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import regex as re\n",
    "\n",
    "text = \"Good job!üëçI‚Äôm so happyüòä\"\n",
    "emoji_text = emoji.demojize(text)\n",
    "\n",
    "# Remove ':' and '_' characters\n",
    "emoji_text_cleaned = emoji_text.replace(':', ' ').replace('_', ' ')\n",
    "\n",
    "print(f\"üîµ Original Text: {text}\\n\")\n",
    "print(f\"üü¢ Text With Handled Emojis: {emoji_text_cleaned}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee66ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f43ef",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ **Tokenization**\n",
    "\n",
    "### ‚ö†Ô∏è **Very Important Step**\n",
    "\n",
    "### üîç **Why:**\n",
    "Machine learning models in NLP typically require numerical input. Tokenization converts text into a format that can be converted into numerical representations (like word embeddings) that models can understand.\n",
    "\n",
    "#### **Example:**\n",
    "Let's say we have the sentence: `The quick brown fox jumps over the lazy dog.`\n",
    "Word Tokenization:\n",
    "> [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In the search engine, \"best Italian restaurants near me\" is tokenized into the words: [\"best\", \"Italian\", \"restaurants\", \"near\", \"me\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a10828",
   "metadata": {},
   "source": [
    "#### Tokenization with `spacy.blank(\"en\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2221d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original:  Hello, world! This is a test sentence. Let's see how it works. \n",
      "\n",
      "üü¢ After Tokenization Using spacy.blank('en'):  ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', '.', 'Let', \"'s\", 'see', 'how', 'it', 'works', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Create a blank English NLP object (only tokenizer) No lemmatization, NER, POS & Tagging\n",
    "nlp_blank = spacy.blank(\"en\")\n",
    "\n",
    "# Input text\n",
    "text = \"Hello, world! This is a test sentence. Let's see how it works.\"\n",
    "\n",
    "# Tokenize using blank pipeline\n",
    "doc_blank = nlp_blank(text)\n",
    "\n",
    "# Print each token\n",
    "token_list = [token.text for token in doc_blank]\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(f\"üü¢ After Tokenization Using spacy.blank('en'): \", token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f7c66",
   "metadata": {},
   "source": [
    "#### Tokenization with `spacy.load(\"en_core_web_sm\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65bf405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original:  Hello, world! This is a test sentence. Let's see how it works. \n",
      "\n",
      "üü¢ After Tokenization Using English Model:  ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', '.', 'Let', \"'s\", 'see', 'how', 'it', 'works', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load pretrained small English model (includes tokenizer, POS, NER, etc.)\n",
    "nlp_full = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input text\n",
    "text = \"Hello, world! This is a test sentence. Let's see how it works.\"\n",
    "\n",
    "# Process text\n",
    "doc_full = nlp_full(text)\n",
    "\n",
    "# Print each token\n",
    "token_list = [token.text for token in doc_full]\n",
    "\n",
    "print(\"üîµ Original Text: \", text, \"\\n\")\n",
    "print(\"üü¢ After Tokenization Using English Model: \", token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b63df1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e6015",
   "metadata": {},
   "source": [
    "## üîü **Stemming**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Stemming reduces words to a base/root form. It‚Äôs fast and works well in information retrieval systems.\n",
    "\n",
    "- May produce non-words (e.g., ‚Äústudies‚Äù ‚Üí ‚Äústudi‚Äù).\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- Used in search engines (e.g., ‚Äúsearching‚Äù, ‚Äúsearched‚Äù, ‚Äúsearches‚Äù ‚Üí ‚Äúsearch‚Äù) to show relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed7ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original:  ['Organizing', 'events', 'helps', 'communities', 'grow', 'and', 'stay', 'connected', '.'] \n",
      "\n",
      "üü¢ After Stemming Using PorterStemmer:  ['organ', 'event', 'help', 'commun', 'grow', 'and', 'stay', 'connect', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Load spaCy (for tokenization)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Three well-known stemming algorithms are the Porter stemmer, Snowball stemmer, and Lancaster stemmer\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Input text\n",
    "text = \"Organizing events helps communities grow and stay connected.\"\n",
    "\n",
    "# Tokenize using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Apply stemming using NLTK\n",
    "stemmed_tokens = [stemmer.stem(token.text) for token in doc]\n",
    "\n",
    "# Output\n",
    "print(\"üîµ Original Text: \", [token.text for token in doc], \"\\n\")\n",
    "print(\"üü¢ After Stemming Using PorterStemmer: \", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67c16e",
   "metadata": {},
   "source": [
    "#### **The 3 Main NLTK Stemmers + Lemmatization**\n",
    "\n",
    "| Technique            | Aggressiveness   | Language Support                 | Speed                     | Output Quality               | Notes                                                               |\n",
    "| -------------------- | ---------------- | -------------------------------- | ------------------------- | ---------------------------- | ------------------------------------------------------------------- |\n",
    "| **PorterStemmer**    | ‚úÖ Moderate       | English only                     | ‚úÖ Fast                    | ‚úÖ Good, widely used          | Most commonly used, balanced stemming algorithm                     |\n",
    "| **LancasterStemmer** | üî• Very high     | English only                     | ‚úÖ Fast                    | ‚ùå Over-stems words           | Very aggressive ‚Üí `maximum` ‚Üí `maxim`                       |\n",
    "| **SnowballStemmer**  | üü¢ Balanced      | ‚úÖ Multiple langs                 | ‚úÖ Fast                    | ‚úÖ Clean, improved            | Also called ‚ÄúPorter2‚Äù ‚Äî better, more consistent than Porter         |\n",
    "| **Lemmatization**    | ‚ùå Not aggressive | ‚úÖ Multiple langs (WordNet/spaCy) | ‚ö†Ô∏è Slower (context-aware) | ‚úÖ Real words & context-aware | Uses grammar + vocabulary, returns meaningful dictionary base forms |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd86751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original:  The organization organized organized events efficiently. \n",
      "\n",
      "üü¢ PorterStemmer:  ['the', 'organ', 'organ', 'organ', 'event', 'effici', '.'] \n",
      "\n",
      "üî¥ LancasterStemmer:  ['the', 'org', 'org', 'org', 'ev', 'efficy', '.'] \n",
      "\n",
      "üü° SnowballStemmer:  ['the', 'organ', 'organ', 'organ', 'event', 'effici', '.'] \n",
      "\n",
      "üü£ Lemmatization:  ['the', 'organization', 'organize', 'organize', 'event', 'efficiently', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"The organization organized organized events efficiently.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = nlp(text)\n",
    "\n",
    "# Initialize stemmers\n",
    "porter = PorterStemmer() # Balanced stemmer\n",
    "lancaster = LancasterStemmer() # aggressive stemmer\n",
    "snowball = SnowballStemmer(\"english\") # Also Called Porter2\n",
    "\n",
    "# Apply each stemmer\n",
    "porter_result = [porter.stem(word.text) for word in tokens]\n",
    "lancaster_result = [lancaster.stem(word.text) for word in tokens]\n",
    "snowball_result = [snowball.stem(word.text) for word in tokens]\n",
    "lemmized_result = [word.lemma_ for word in tokens]  # Lemmatization using spaCy\n",
    "\n",
    "print(\"üîµ Original Text: \", tokens, \"\\n\")\n",
    "print(\"üü¢ PorterStemmer: \", porter_result, \"\\n\")\n",
    "print(\"üî¥ LancasterStemmer: \", lancaster_result, \"\\n\")\n",
    "print(\"üü° SnowballStemmer: \", snowball_result, \"\\n\")\n",
    "print(\"üü£ Lemmatization: \", lemmized_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8fc55b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c15c0",
   "metadata": {},
   "source": [
    "## üî¢ **Lemmatization**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Like stemming, but linguistically accurate.\n",
    "\n",
    "- Uses vocabulary and grammar rules to return the correct base word.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- For grammar-sensitive tasks like question answering or summarization, ‚Äúwas‚Äù should be lemmatized to ‚Äúbe‚Äù, not ‚Äúwa‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2227650",
   "metadata": {},
   "source": [
    "#### **Comparison: Lemmatization in spaCy vs NLTK**\n",
    "| Feature           | **spaCy**                         | **NLTK (WordNetLemmatizer)**         |\n",
    "| ----------------- | --------------------------------- | ------------------------------------ |\n",
    "| Accuracy          | ‚úÖ High (uses POS + context)       | ‚ö†Ô∏è Medium (needs manual POS mapping) |\n",
    "| Language model    | `en_core_web_sm` or larger models | WordNet dictionary                   |\n",
    "| POS Tag Awareness | ‚úÖ Automatic                       | ‚ö†Ô∏è Must provide POS manually         |\n",
    "| Output Quality    | ‚úÖ Real, accurate base forms       | ‚úÖ Good, but context-blind sometimes  |\n",
    "| Ease of Use       | ‚úÖ Very easy                       | ‚ö†Ô∏è Slightly complex (POS conversion) |\n",
    "| Performance       | ‚úÖ Fast and optimized              | ‚úÖ Fast but rule-based                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ea81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original:  The organization organized organized events efficiently. \n",
      "\n",
      "üü¢ After Lemmatization:  the child be run and well organize event every week . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"The children were running and better organized events every week.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Lemmatize with POS awareness\n",
    "lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "# Output\n",
    "print(\"üîµ Original Text: \", tokens, \"\\n\")\n",
    "print(\"üü¢ After Lemmatization: \", \" \".join(lemmatized_tokens), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0caad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Lemmas: ['The', 'child', 'be', 'run', 'and', 'good', 'organize', 'event', 'every', 'week', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# POS tag conversion\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "text = \"The children were running and better organized events every week.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "result = [tokens.text for tokens in doc]\n",
    "tagged = pos_tag(result)\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n",
    "print(\"NLTK Lemmas:\", lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2105d408",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d646831",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary Table\n",
    "\n",
    "| Step                | Purpose                                                                 | Use Case Examples                                     |\n",
    "|---------------------|-------------------------------------------------------------------------|--------------------------------------------------------|\n",
    "| Lowercasing          | Normalize text casing                                                   | Text classification, search                           |\n",
    "| Remove HTML Tags     | remove unwanted HTML tags                                                | Web scraping, email cleaning                          |\n",
    "| Remove URLs          | Remove unnecessary web links                                             | Social media, forums                                  |\n",
    "| Remove Punctuation   | Clean punctuation noise                                                  | Preprocessing for BoW/TF-IDF                          |\n",
    "| Chat Word Treatment  | Convert slang to standard English like GD to Good Night                  | Chatbots, social media analysis                       |\n",
    "| Spelling Correction  | Fix typos for vocabulary consistency                                     | User reviews, text input                              |\n",
    "| Remove Stop Words    | Focus on meaningful words                                                 | Summarization, topic modeling                         |\n",
    "| Handle Emojis        | Preserve or convert emojis based on task                                | Sentiment analysis                                    |\n",
    "| Tokenization         | Break down text into tokens                                              | All NLP tasks                                         |\n",
    "| Stemming             | Reduce word forms to base/root                                           | Search engines, topic modeling                        |\n",
    "| Lemmatization        | Get accurate root word using grammar                                     | Parsing, QA systems, deep learning models             |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
