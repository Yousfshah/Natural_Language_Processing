{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8b6d61",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f888d2",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ **Lowercasing**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Words like \"Good\", \"GOOD\", and \"good\" are semantically identical but are treated as separate tokens if not lowercased.\n",
    "\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In a product review system, \"Excellent\" and \"excellent\" should be considered the same sentiment word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbb363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: Good Morning EVERYONE! Let's Start Our NLP Journey. \n",
      "\n",
      "üü¢ Preprocessed: good morning everyone! let's start our nlp journey.\n"
     ]
    }
   ],
   "source": [
    "text = \"Good Morning EVERYONE! Let's Start Our NLP Journey.\"\n",
    "lowercased = text.lower()\n",
    "\n",
    "print(\"üîµ Original:\", text, \"\\n\")\n",
    "print(\"üü¢ Preprocessed:\", lowercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a091a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f3aa0",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ **Remove HTML Tags**\n",
    "\n",
    "### üîç **Why:**\n",
    "- HTML is markup used for layout, not meaning.\n",
    "\n",
    "- If you're scraping websites (news, blogs), tags like div tag, span tag add noise.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- When extracting articles from news sites, you'll find lots of formatting tags. Models get confused by p tag, a href tag etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ebc3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: <div>Hello <b>World</b>! NLP is <i>awesome</i>.</div> \n",
      "\n",
      "üü¢ Preprocessed with BS4: Hello World! NLP is awesome. \n",
      "\n",
      "üü¢ Preprocessed with regex: Hello World! NLP is awesome.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "\n",
    "# sample HTML text\n",
    "text = \"<div>Hello <b>World</b>! NLP is <i>awesome</i>.</div>\"\n",
    "\n",
    "# Using BeautifulSoup to remove HTML tags\n",
    "cleaned = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# Using regex to remove HTML tags\n",
    "re_pattern = \"<(?:\\\"[^\\\"]*\\\"['\\\"]*|'[^']*'['\\\"]*|[^'\\\">])+>\" # # Remove all HTML tags from a string\n",
    "cleaned_re = re.sub(re_pattern, '', text)\n",
    "# cleaned = cleaned.strip()\n",
    "\n",
    "print(\"üîµ Original:\", text, \"\\n\")\n",
    "print(\"üü¢ Preprocessed with BS4:\", cleaned, \"\\n\")\n",
    "print(\"üü¢ Preprocessed with regex:\", cleaned_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f8ff7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316925a",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ **Remove URLs**\n",
    "\n",
    "### üîç **Why:**\n",
    "- URLs usually don‚Äôt carry semantic meaning.\n",
    "\n",
    "- They are high variance strings (each one is unique) and hurt model generalization.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In a tweet like ‚ÄúCheck this out üëâ https://xyz.com‚Äù, we care more about the sentiment or emotion, not the URL itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0ffdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: Visit our website at https://www.example.com/page.html or check out www.another-site.org. You can also find info at mydomain.net/info. \n",
      "\n",
      "üü¢ Preprocessed: Visit our website at  or check out  You can also find info at \n"
     ]
    }
   ],
   "source": [
    "# it will remove any type of URL from the text just apply on your text\n",
    "import regex as re\n",
    "\n",
    "# Example usage:\n",
    "text = \"Visit our website at https://www.example.com/page.html or check out www.another-site.org. You can also find info at mydomain.net/info.\"\n",
    "\n",
    "# Regex pattern to match various URL formats (http, https, www, without scheme)\n",
    "url_pattern = re.compile(\n",
    "    r'https?://[^\\s/$.?#].[^\\s]*'  # Matches http/https URLs\n",
    "    r'|www\\.[^\\s/$.?#].[^\\s]*'    # Matches www. URLs\n",
    "    r'|[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}(?:/[^\\s]*)?' # Matches domain-only URLs like example.com\n",
    ")\n",
    "\n",
    "# Remove URLs from the text\n",
    "text_without_urls= url_pattern.sub(r'', text)\n",
    "\n",
    "print(\"üîµ Original:\", text, \"\\n\")\n",
    "print(\"üü¢ Preprocessed:\", text_without_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d70b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752634f5",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ **Remove Punctuation**\n",
    "\n",
    "### üîç **Why:**\n",
    "-  Punctuation marks like `!`, `.`, `?` create unnecessary tokens.\n",
    "\n",
    "- Removing them helps reduce noise, especially in tasks like text classification or topic modeling.\n",
    "\n",
    "| Method                               | Removes ASCII | Removes Unicode | Customizable |\n",
    "| ------------------------------------ | ------------- | --------------- | ------------ |\n",
    "| `string.punctuation` + `translate()` | ‚úÖ Yes         | ‚ùå No            | ‚ùå Limited    |\n",
    "| `re.sub(r'\\p{P}+', '', text)`       | ‚úÖ Yes         | ‚úÖ Yes           | ‚úÖ Yes        |\n",
    "\n",
    "- string.punctuation only covers ASCII punctuation & returns: <div style=color:yellow;>!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~</div>\n",
    "\n",
    "- Unicode symbols like  `‚Äú`, `‚Äî`, and `‚Ä¶` are not included in string.punctuation, so we need to add them manually or using regex\n",
    "\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- For spam detection or document classification, punctuation doesn‚Äôt usually help (unless you‚Äôre analyzing writing style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b850b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: Hello!!! How are you??? I'm fine:) ‚ÄúGreat‚Äù‚Äî yes‚Ä¶ \n",
      "\n",
      "üü¢ Preprocessed: Hello How are you Im fine Great yes\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = \"Hello!!! How are you??? I'm fine:) ‚ÄúGreat‚Äù‚Äî yes‚Ä¶\"\n",
    "no_punc = text.translate(str.maketrans('', '', string.punctuation + '‚Äú‚Äù‚Äî‚Ä¶'))\n",
    "\n",
    "print(\"üîµ Original:\", text, \"\\n\")\n",
    "print(\"üü¢ Preprocessed:\", no_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad8e9421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: Hello!!! How are you??? I'm fine:) ‚ÄúGreat‚Äù‚Äî yes‚Ä¶ \n",
      "\n",
      "üü¢ Preprocessed: Hello How are you Im fine Great yes\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "text = \"Hello!!! How are you??? I'm fine:) ‚ÄúGreat‚Äù‚Äî yes‚Ä¶\"\n",
    "no_punc = re.sub(r'\\p{P}+', '', text)\n",
    "\n",
    "print(\"üîµ Original:\", text, \"\\n\")\n",
    "print(\"üü¢ Preprocessed:\", no_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e21be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5094c72",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ **Chat Word Treatment (e.g., GN ‚Üí Good Night)**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Chat language is full of abbreviations: \"lol\", \"idk\", \"smh\".\n",
    "\n",
    "- They need to be normalized to standard English for models to understand.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In customer support or social media, replacing abbreviations like `\"brb\"` with `\"be right back\"` helps understand the message better.\n",
    "\n",
    "### üó£Ô∏è **Slang Words**\n",
    "- The following two GitHub repositories provide comprehensive lists of slang words and their standard English equivalents.These resources are useful for expanding chat abbreviations (e.g., \"GN\" ‚Üí \"Good Night\") during text preprocessing\n",
    "\n",
    "    - [Repo1](https://github.com/ipekdk/abbreviation-list-english)\n",
    "    - [Repo2](https://github.com/bodhwani/NLP-VIT-BOT/blob/master/slangs.csv)\n",
    "    - [Comman Slang Words](https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb4d81",
   "metadata": {},
   "source": [
    "#### Replace Slang words with the help of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "79551381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Text:\n",
      "BRB guys, lol this is funny. ttyl!\n",
      "\n",
      "üü¢ Preprocessed Text:\n",
      "Be Right Back guys , Laughing Out Loud this is funny . Talk To You Later !\n"
     ]
    }
   ],
   "source": [
    "# Use spaCy for tokenization and keep punctuation marks\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"BRB guys, lol this is funny. ttyl!\"\n",
    "# Ensure 'nlp' is already loaded with spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)  # 'text' variable is already defined\n",
    "\n",
    "# Load slang dictionary from CSV\n",
    "chat_dict = pd.read_csv(\"slangs.csv\")  # Must have columns 'Abbr', 'Fullform'\n",
    "slang_map = dict(zip(chat_dict['Abbr'].str.lower(), chat_dict['Fullform']))\n",
    "\n",
    "# Replace slangs with full forms, preserving punctuation\n",
    "processed_tokens = []\n",
    "for token in doc:\n",
    "    token_text = token.text\n",
    "    lower_token = token_text.lower()\n",
    "    if lower_token in slang_map:\n",
    "        processed_tokens.extend(slang_map[lower_token].split())\n",
    "    else:\n",
    "        processed_tokens.append(token_text)\n",
    "\n",
    "preprocessed_text = \" \".join(processed_tokens)\n",
    "\n",
    "print(\"üîµ Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nüü¢ Preprocessed Text:\")\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724952e5",
   "metadata": {},
   "source": [
    "#### Replace Slang word with the help of Split\n",
    "\n",
    "- you can see the difference `','` , `'.'` and `'!'` present in original text but not in preprocessed text using split but in the case of tokenization there is no missingness of `','` , `'.'` and `'!'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "55030324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: BRB guys, lol this is funny. ttyl! \n",
      "\n",
      "üü¢ Preprocessed: Be Right Back guys Laughing Out Loud this is funny Talk To You Later\n"
     ]
    }
   ],
   "source": [
    "# just apply to your text this comprehenisve list of slang abbreviations\n",
    "# Make sure to have a CSV file named 'slangs.csv' with columns 'Abbr\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re  # Use the 'regex' module for Unicode property escapes\n",
    "\n",
    "# Load slang dictionary from CSV\n",
    "chat_dict = pd.read_csv(\"slangs.csv\")  # Must have columns 'Abbr', 'Fullform'\n",
    "slang_map = dict(zip(chat_dict['Abbr'].str.lower(), chat_dict['Fullform']))\n",
    "\n",
    "# Sample chat-like text\n",
    "text = \"BRB guys, lol this is funny. ttyl!\"\n",
    "\n",
    "# Remove punctuation using regex for matching text\n",
    "text_clean = re.sub(r'\\p{P}+', '', text)\n",
    "\n",
    "# Tokenize by whitespace\n",
    "words = text_clean.split()\n",
    "\n",
    "# Replace slangs\n",
    "chat_fixed = []\n",
    "for word in words:\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in slang_map:\n",
    "        chat_fixed.append(slang_map[word_lower])\n",
    "    else:\n",
    "        chat_fixed.append(word)\n",
    "\n",
    "# Join the cleaned text\n",
    "cleaned_text = \" \".join(chat_fixed) # The join() method takes all items in an iterable and joins them into one string. A string must be specified as the separator\n",
    "\n",
    "# Output\n",
    "print(\"üîµ Original:\", text, \"\\n\")\n",
    "print(\"üü¢ Preprocessed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c83a6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af2dbc",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ **Spelling Correction**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Typos are common in user input (especially social media).\n",
    "\n",
    "- Words like ‚Äúbeleive‚Äù won‚Äôt match ‚Äúbelieve‚Äù in dictionaries or embeddings.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- Spell correction boosts chatbot understanding and auto-correction in search bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0145f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: I realy love natral language prosesing. \n",
      "\n",
      "üü¢ Preprocessed (autocorrect): I really love natural language crossing. \n",
      "\n",
      "üü¢ Preprocessed (TextBlob): I really love natural language pressing. \n",
      "\n",
      "üü¢ Preprocessed (pyspellchecker): I really love natural language prosesing.\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "text = \"I realy love natral language prosesing.\"\n",
    "\n",
    "# Autocorrect\n",
    "corrected_autocorrect = spell(text)\n",
    "\n",
    "# TextBlob\n",
    "corrected_textblob = str(TextBlob(text).correct())\n",
    "\n",
    "# SpellChecker\n",
    "spellchecker = SpellChecker()\n",
    "# Tokenize the text for word-level correction\n",
    "tokens = text.split()\n",
    "corrected_pyspellchecker = ' '.join([spellchecker.correction(word) or word for word in tokens])\n",
    "\n",
    "\n",
    "print(\"üîµ Original:\", text, \"\\n\")\n",
    "print(\"üü¢ Preprocessed (autocorrect):\", corrected_autocorrect, \"\\n\")\n",
    "print(\"üü¢ Preprocessed (TextBlob):\", corrected_textblob, \"\\n\")\n",
    "print(\"üü¢ Preprocessed (pyspellchecker):\", corrected_pyspellchecker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e06efd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f78e50",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ **Remove Stop Words**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Stopwords like \"the\", \"is\", \"at\" are function words, not content words.\n",
    "\n",
    "- Removing them improves focus on important tokens.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In a movie review, ‚ÄúThe movie was absolutely fantastic‚Äù, word ‚Äúfantastic‚Äù carries the sentiment, not ‚Äúthe‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764e2b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence', 'stop', 'words']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words from tokens using spaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"This is a sample sentence with some stop words\"\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb1854a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n"
     ]
    }
   ],
   "source": [
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"This is a sample sentence.\"\n",
    "tokens = word_tokenize(text)\n",
    "filtered = [word for word in tokens if word.lower() not in stopwords.words('english')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20317b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample big text\n",
    "text = \"This is a sample sentence with some stop words. It contains multiple sentences and should be filtered accordingly.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1368b856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: sample sentence stop words . contains multiple sentences filtered accordingly .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input text\n",
    "text = \"This is a sample sentence with some stop words. It contains multiple sentences and should be filtered accordingly.\"\n",
    "\n",
    "# Tokenize using spaCy\n",
    "tokens = nlp(text)\n",
    "\n",
    "# Filter stopwords using NLTK's list\n",
    "filtered = \" \".join([word.text for word in tokens if word.text.lower() not in stopwords.words('english')])\n",
    "\n",
    "print(\"Filtered Tokens:\", filtered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91ed2e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: sample sentence stop words . contains multiple sentences filtered accordingly .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input text\n",
    "text = \"This is a sample sentence with some stop words. It contains multiple sentences and should be filtered accordingly.\"\n",
    "\n",
    "# Tokenize using spaCy\n",
    "tokens = nlp(text)\n",
    "\n",
    "# Filter stopwords using NLTK's list\n",
    "filtered = \" \".join([token.text for token in tokens if not token.is_stop])\n",
    "\n",
    "print(\"Filtered Tokens:\", filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d63cfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample sentence stop words\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "import spacy\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "text = \"This is a sample sentence with some stop words\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_tokens = \" \".join([token.text for token in doc if not token.is_stop])\n",
    "\n",
    "# Print the text excluding stop words\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd6e4b1",
   "metadata": {},
   "source": [
    "| Feature       | NLTK                             | spaCy                                   |\n",
    "| ------------- | -------------------------------- | --------------------------------------- |\n",
    "| Stopword List | Manually maintained              | Linguistically curated                  |\n",
    "| Tokenization  | Basic (word\\_tokenize)           | Advanced (handles context, punctuation) |\n",
    "| Flexibility   | High (can customize list easily) | High (custom rules, pipelines)          |\n",
    "| Performance   | Lightweight                      | Heavier but more powerful               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc3974",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680edd1",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ **Handling Emojis**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Emojis are sentiment-rich tokens (üòä, üò¢).\n",
    "\n",
    "- You can either remove them, convert to words, or use them as features.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In social media sentiment analysis, üò† and ‚ù§Ô∏è change the tone completely and must be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c2e03834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: Good job!üëçI‚Äôm so happyüòä\n",
      "\n",
      "üü¢ Preprocessed: Good job! thumbs up I‚Äôm so happy smiling face with smiling eyes \n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import regex as re\n",
    "\n",
    "text = \"Good job!üëçI‚Äôm so happyüòä\"\n",
    "emoji_text = emoji.demojize(text)\n",
    "\n",
    "# Remove ':' and '_' characters\n",
    "emoji_text_cleaned = emoji_text.replace(':', ' ').replace('_', ' ')\n",
    "\n",
    "print(f\"üîµ Original: {text}\\n\")\n",
    "print(f\"üü¢ Preprocessed: {emoji_text_cleaned}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee66ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f43ef",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ **Tokenization**\n",
    "\n",
    "### ‚ö†Ô∏è **Very Important Step**\n",
    "\n",
    "### üîç **Why:**\n",
    "Machine learning models in NLP typically require numerical input. Tokenization converts text into a format that can be converted into numerical representations (like word embeddings) that models can understand.\n",
    "\n",
    "#### **Example:**\n",
    "Let's say we have the sentence: `The quick brown fox jumps over the lazy dog.`\n",
    "Word Tokenization:\n",
    "> [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In the search engine, \"best Italian restaurants near me\" is tokenized into the words: [\"best\", \"Italian\", \"restaurants\", \"near\", \"me\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenization splits sentences into words.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"üîµ Original:\", text)\n",
    "print(\"üü¢ Preprocessed:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b63df1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e6015",
   "metadata": {},
   "source": [
    "## üîü **Stemming**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Stemming reduces words to a base/root form. It‚Äôs fast and works well in information retrieval systems.\n",
    "\n",
    "- May produce non-words (e.g., ‚Äústudies‚Äù ‚Üí ‚Äústudi‚Äù).\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- Used in search engines (e.g., ‚Äúsearching‚Äù, ‚Äúsearched‚Äù, ‚Äúsearches‚Äù ‚Üí ‚Äúsearch‚Äù) to show relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b3186",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "text = \"We are studying various stemmed words: running, studies, cries\"\n",
    "tokens = word_tokenize(text)\n",
    "stemmed = [stemmer.stem(w) for w in tokens]\n",
    "\n",
    "print(\"üîµ Original:\", tokens)\n",
    "print(\"üü¢ Preprocessed:\", stemmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8fc55b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c15c0",
   "metadata": {},
   "source": [
    "## üî¢ **Lemmatization**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Like stemming, but linguistically accurate.\n",
    "\n",
    "- Uses vocabulary and grammar rules to return the correct base word.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- For grammar-sensitive tasks like question answering or summarization, ‚Äúwas‚Äù should be lemmatized to ‚Äúbe‚Äù, not ‚Äúwa‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98194606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"The children were playing with their better toys.\"\n",
    "doc = nlp(text)\n",
    "lemmatized = [token.lemma_ for token in doc]\n",
    "\n",
    "print(\"üîµ Original:\", text)\n",
    "print(\"üü¢ Preprocessed:\", lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042e66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d646831",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary Table\n",
    "\n",
    "| Step                | Purpose                                                                 | Use Case Examples                                     |\n",
    "|---------------------|-------------------------------------------------------------------------|--------------------------------------------------------|\n",
    "| Lowercasing          | Normalize text casing                                                   | Text classification, search                           |\n",
    "| Remove HTML Tags     | remove unwanted HTML tags                                                | Web scraping, email cleaning                          |\n",
    "| Remove URLs          | Remove unnecessary web links                                             | Social media, forums                                  |\n",
    "| Remove Punctuation   | Clean punctuation noise                                                  | Preprocessing for BoW/TF-IDF                          |\n",
    "| Chat Word Treatment  | Convert slang to standard English like GD to Good Night                  | Chatbots, social media analysis                       |\n",
    "| Spelling Correction  | Fix typos for vocabulary consistency                                     | User reviews, text input                              |\n",
    "| Remove Stop Words    | Focus on meaningful words                                                 | Summarization, topic modeling                         |\n",
    "| Handle Emojis        | Preserve or convert emojis based on task                                | Sentiment analysis                                    |\n",
    "| Tokenization         | Break down text into tokens                                              | All NLP tasks                                         |\n",
    "| Stemming             | Reduce word forms to base/root                                           | Search engines, topic modeling                        |\n",
    "| Lemmatization        | Get accurate root word using grammar                                     | Parsing, QA systems, deep learning models             |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
