{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8b6d61",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f888d2",
   "metadata": {},
   "source": [
    "## 1️⃣ **Lowercasing**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Words like \"Good\", \"GOOD\", and \"good\" are semantically identical but are treated as separate tokens if not lowercased.\n",
    "\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In a product review system, \"Excellent\" and \"excellent\" should be considered the same sentiment word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbb363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: Good Morning EVERYONE! Let's Start Our NLP Journey. \n",
      "\n",
      "🟢 Preprocessed: good morning everyone! let's start our nlp journey.\n"
     ]
    }
   ],
   "source": [
    "text = \"Good Morning EVERYONE! Let's Start Our NLP Journey.\"\n",
    "lowercased = text.lower()\n",
    "\n",
    "print(\"🔵 Original:\", text, \"\\n\")\n",
    "print(\"🟢 Preprocessed:\", lowercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a091a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f3aa0",
   "metadata": {},
   "source": [
    "## 2️⃣ **Remove HTML Tags**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- HTML is markup used for layout, not meaning.\n",
    "\n",
    "- If you're scraping websites (news, blogs), tags like div tag, span tag add noise.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- When extracting articles from news sites, you'll find lots of formatting tags. Models get confused by p tag, a href tag etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ebc3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: <div>Hello <b>World</b>! NLP is <i>awesome</i>.</div> \n",
      "\n",
      "🟢 Preprocessed with BS4: Hello World! NLP is awesome. \n",
      "\n",
      "🟢 Preprocessed with regex: Hello World! NLP is awesome.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "\n",
    "# sample HTML text\n",
    "text = \"<div>Hello <b>World</b>! NLP is <i>awesome</i>.</div>\"\n",
    "\n",
    "# Using BeautifulSoup to remove HTML tags\n",
    "cleaned = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# Using regex to remove HTML tags\n",
    "re_pattern = \"<(?:\\\"[^\\\"]*\\\"['\\\"]*|'[^']*'['\\\"]*|[^'\\\">])+>\" # # Remove all HTML tags from a string\n",
    "cleaned_re = re.sub(re_pattern, '', text)\n",
    "# cleaned = cleaned.strip()\n",
    "\n",
    "print(\"🔵 Original:\", text, \"\\n\")\n",
    "print(\"🟢 Preprocessed with BS4:\", cleaned, \"\\n\")\n",
    "print(\"🟢 Preprocessed with regex:\", cleaned_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f8ff7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316925a",
   "metadata": {},
   "source": [
    "## 3️⃣ **Remove URLs**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- URLs usually don’t carry semantic meaning.\n",
    "\n",
    "- They are high variance strings (each one is unique) and hurt model generalization.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In a tweet like “Check this out 👉 https://xyz.com”, we care more about the sentiment or emotion, not the URL itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0ffdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: Visit our website at https://www.example.com/page.html or check out www.another-site.org. You can also find info at mydomain.net/info. \n",
      "\n",
      "🟢 Preprocessed: Visit our website at  or check out  You can also find info at \n"
     ]
    }
   ],
   "source": [
    "# it will remove any type of URL from the text just apply on your text\n",
    "import regex as re\n",
    "\n",
    "# Example usage:\n",
    "text = \"Visit our website at https://www.example.com/page.html or check out www.another-site.org. You can also find info at mydomain.net/info.\"\n",
    "\n",
    "# Regex pattern to match various URL formats (http, https, www, without scheme)\n",
    "url_pattern = re.compile(\n",
    "    r'https?://[^\\s/$.?#].[^\\s]*'  # Matches http/https URLs\n",
    "    r'|www\\.[^\\s/$.?#].[^\\s]*'    # Matches www. URLs\n",
    "    r'|[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}(?:/[^\\s]*)?' # Matches domain-only URLs like example.com\n",
    ")\n",
    "\n",
    "# Remove URLs from the text\n",
    "text_without_urls= url_pattern.sub(r'', text)\n",
    "\n",
    "print(\"🔵 Original:\", text, \"\\n\")\n",
    "print(\"🟢 Preprocessed:\", text_without_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d70b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752634f5",
   "metadata": {},
   "source": [
    "## 4️⃣ **Remove Punctuation**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "-  Punctuation marks like `!`, `.`, `?` create unnecessary tokens.\n",
    "\n",
    "- Removing them helps reduce noise, especially in tasks like text classification or topic modeling.\n",
    "\n",
    "| Method                               | Removes ASCII | Removes Unicode | Customizable |\n",
    "| ------------------------------------ | ------------- | --------------- | ------------ |\n",
    "| `string.punctuation` + `translate()` | ✅ Yes         | ❌ No            | ❌ Limited    |\n",
    "| `re.sub(r'\\p{P}+', '', text)`       | ✅ Yes         | ✅ Yes           | ✅ Yes        |\n",
    "\n",
    "- string.punctuation only covers ASCII punctuation & returns: <div style=color:yellow;>!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~</div>\n",
    "\n",
    "- Unicode symbols like  `“`, `—`, and `…` are not included in string.punctuation, so we need to add them manually or using regex\n",
    "\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- For spam detection or document classification, punctuation doesn’t usually help (unless you’re analyzing writing style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b850b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: Hello!!! How are you??? I'm fine:) “Great”— yes… \n",
      "\n",
      "🟢 Preprocessed: Hello How are you Im fine Great yes\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = \"Hello!!! How are you??? I'm fine:) “Great”— yes…\"\n",
    "no_punc = text.translate(str.maketrans('', '', string.punctuation + '“”—…'))\n",
    "\n",
    "print(\"🔵 Original:\", text, \"\\n\")\n",
    "print(\"🟢 Preprocessed:\", no_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad8e9421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: Hello!!! How are you??? I'm fine:) “Great”— yes… \n",
      "\n",
      "🟢 Preprocessed: Hello How are you Im fine Great yes\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "text = \"Hello!!! How are you??? I'm fine:) “Great”— yes…\"\n",
    "no_punc = re.sub(r'\\p{P}+', '', text)\n",
    "\n",
    "print(\"🔵 Original:\", text, \"\\n\")\n",
    "print(\"🟢 Preprocessed:\", no_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e21be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5094c72",
   "metadata": {},
   "source": [
    "## 5️⃣ **Chat Word Treatment (e.g., GN → Good Night)**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Chat language is full of abbreviations: \"lol\", \"idk\", \"smh\".\n",
    "\n",
    "- They need to be normalized to standard English for models to understand.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In customer support or social media, replacing abbreviations like `\"brb\"` with `\"be right back\"` helps understand the message better.\n",
    "\n",
    "### 🗣️ **Slang Words**\n",
    "- The following two GitHub repositories provide comprehensive lists of slang words and their standard English equivalents.These resources are useful for expanding chat abbreviations (e.g., \"GN\" → \"Good Night\") during text preprocessing\n",
    "\n",
    "    - [Repo1](https://github.com/ipekdk/abbreviation-list-english)\n",
    "    - [Repo2](https://github.com/bodhwani/NLP-VIT-BOT/blob/master/slangs.csv)\n",
    "    - [Comman Slang Words](https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb4d81",
   "metadata": {},
   "source": [
    "#### Replace Slang words with the help of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "79551381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original Text:\n",
      "BRB guys, lol this is funny. ttyl!\n",
      "\n",
      "🟢 Preprocessed Text:\n",
      "Be Right Back guys , Laughing Out Loud this is funny . Talk To You Later !\n"
     ]
    }
   ],
   "source": [
    "# Use spaCy for tokenization and keep punctuation marks\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"BRB guys, lol this is funny. ttyl!\"\n",
    "# Ensure 'nlp' is already loaded with spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)  # 'text' variable is already defined\n",
    "\n",
    "# Load slang dictionary from CSV\n",
    "chat_dict = pd.read_csv(\"slangs.csv\")  # Must have columns 'Abbr', 'Fullform'\n",
    "slang_map = dict(zip(chat_dict['Abbr'].str.lower(), chat_dict['Fullform']))\n",
    "\n",
    "# Replace slangs with full forms, preserving punctuation\n",
    "processed_tokens = []\n",
    "for token in doc:\n",
    "    token_text = token.text\n",
    "    lower_token = token_text.lower()\n",
    "    if lower_token in slang_map:\n",
    "        processed_tokens.extend(slang_map[lower_token].split())\n",
    "    else:\n",
    "        processed_tokens.append(token_text)\n",
    "\n",
    "preprocessed_text = \" \".join(processed_tokens)\n",
    "\n",
    "print(\"🔵 Original Text:\")\n",
    "print(text)\n",
    "print(\"\\n🟢 Preprocessed Text:\")\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724952e5",
   "metadata": {},
   "source": [
    "#### Replace Slang word with the help of Split\n",
    "\n",
    "- you can see the difference `','` , `'.'` and `'!'` present in original text but not in preprocessed text using split but in the case of tokenization there is no missingness of `','` , `'.'` and `'!'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "55030324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: BRB guys, lol this is funny. ttyl! \n",
      "\n",
      "🟢 Preprocessed: Be Right Back guys Laughing Out Loud this is funny Talk To You Later\n"
     ]
    }
   ],
   "source": [
    "# just apply to your text this comprehenisve list of slang abbreviations\n",
    "# Make sure to have a CSV file named 'slangs.csv' with columns 'Abbr\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re  # Use the 'regex' module for Unicode property escapes\n",
    "\n",
    "# Load slang dictionary from CSV\n",
    "chat_dict = pd.read_csv(\"slangs.csv\")  # Must have columns 'Abbr', 'Fullform'\n",
    "slang_map = dict(zip(chat_dict['Abbr'].str.lower(), chat_dict['Fullform']))\n",
    "\n",
    "# Sample chat-like text\n",
    "text = \"BRB guys, lol this is funny. ttyl!\"\n",
    "\n",
    "# Remove punctuation using regex for matching text\n",
    "text_clean = re.sub(r'\\p{P}+', '', text)\n",
    "\n",
    "# Tokenize by whitespace\n",
    "words = text_clean.split()\n",
    "\n",
    "# Replace slangs\n",
    "chat_fixed = []\n",
    "for word in words:\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in slang_map:\n",
    "        chat_fixed.append(slang_map[word_lower])\n",
    "    else:\n",
    "        chat_fixed.append(word)\n",
    "\n",
    "# Join the cleaned text\n",
    "cleaned_text = \" \".join(chat_fixed) # The join() method takes all items in an iterable and joins them into one string. A string must be specified as the separator\n",
    "\n",
    "# Output\n",
    "print(\"🔵 Original:\", text, \"\\n\")\n",
    "print(\"🟢 Preprocessed:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c83a6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af2dbc",
   "metadata": {},
   "source": [
    "## 6️⃣ **Spelling Correction**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Typos are common in user input (especially social media).\n",
    "\n",
    "- Words like “beleive” won’t match “believe” in dictionaries or embeddings.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- Spell correction boosts chatbot understanding and auto-correction in search bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0145f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: I realy love natral language prosesing. \n",
      "\n",
      "🟢 Preprocessed (autocorrect): I really love natural language crossing. \n",
      "\n",
      "🟢 Preprocessed (TextBlob): I really love natural language pressing. \n",
      "\n",
      "🟢 Preprocessed (pyspellchecker): I really love natural language prosesing.\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "text = \"I realy love natral language prosesing.\"\n",
    "\n",
    "# Autocorrect\n",
    "corrected_autocorrect = spell(text)\n",
    "\n",
    "# TextBlob\n",
    "corrected_textblob = str(TextBlob(text).correct())\n",
    "\n",
    "# SpellChecker\n",
    "spellchecker = SpellChecker()\n",
    "# Tokenize the text for word-level correction\n",
    "tokens = text.split()\n",
    "corrected_pyspellchecker = ' '.join([spellchecker.correction(word) or word for word in tokens])\n",
    "\n",
    "\n",
    "print(\"🔵 Original:\", text, \"\\n\")\n",
    "print(\"🟢 Preprocessed (autocorrect):\", corrected_autocorrect, \"\\n\")\n",
    "print(\"🟢 Preprocessed (TextBlob):\", corrected_textblob, \"\\n\")\n",
    "print(\"🟢 Preprocessed (pyspellchecker):\", corrected_pyspellchecker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e06efd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f78e50",
   "metadata": {},
   "source": [
    "## 7️⃣ **Remove Stop Words**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Stopwords like \"the\", \"is\", \"at\" are function words, not content words.\n",
    "\n",
    "- Removing them improves focus on important tokens.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In a movie review, “The movie was absolutely fantastic”, word “fantastic” carries the sentiment, not “the”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764e2b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence', 'stop', 'words']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words from tokens using spaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"This is a sample sentence with some stop words\"\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb1854a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n"
     ]
    }
   ],
   "source": [
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"This is a sample sentence.\"\n",
    "tokens = word_tokenize(text)\n",
    "filtered = [word for word in tokens if word.lower() not in stopwords.words('english')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20317b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample big text\n",
    "text = \"This is a sample sentence with some stop words. It contains multiple sentences and should be filtered accordingly.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1368b856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: sample sentence stop words . contains multiple sentences filtered accordingly .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input text\n",
    "text = \"This is a sample sentence with some stop words. It contains multiple sentences and should be filtered accordingly.\"\n",
    "\n",
    "# Tokenize using spaCy\n",
    "tokens = nlp(text)\n",
    "\n",
    "# Filter stopwords using NLTK's list\n",
    "filtered = \" \".join([word.text for word in tokens if word.text.lower() not in stopwords.words('english')])\n",
    "\n",
    "print(\"Filtered Tokens:\", filtered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91ed2e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: sample sentence stop words . contains multiple sentences filtered accordingly .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input text\n",
    "text = \"This is a sample sentence with some stop words. It contains multiple sentences and should be filtered accordingly.\"\n",
    "\n",
    "# Tokenize using spaCy\n",
    "tokens = nlp(text)\n",
    "\n",
    "# Filter stopwords using NLTK's list\n",
    "filtered = \" \".join([token.text for token in tokens if not token.is_stop])\n",
    "\n",
    "print(\"Filtered Tokens:\", filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d63cfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample sentence stop words\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "import spacy\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "text = \"This is a sample sentence with some stop words\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_tokens = \" \".join([token.text for token in doc if not token.is_stop])\n",
    "\n",
    "# Print the text excluding stop words\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd6e4b1",
   "metadata": {},
   "source": [
    "| Feature       | NLTK                             | spaCy                                   |\n",
    "| ------------- | -------------------------------- | --------------------------------------- |\n",
    "| Stopword List | Manually maintained              | Linguistically curated                  |\n",
    "| Tokenization  | Basic (word\\_tokenize)           | Advanced (handles context, punctuation) |\n",
    "| Flexibility   | High (can customize list easily) | High (custom rules, pipelines)          |\n",
    "| Performance   | Lightweight                      | Heavier but more powerful               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc3974",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680edd1",
   "metadata": {},
   "source": [
    "## 8️⃣ **Handling Emojis**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Emojis are sentiment-rich tokens (😊, 😢).\n",
    "\n",
    "- You can either remove them, convert to words, or use them as features.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In social media sentiment analysis, 😠 and ❤️ change the tone completely and must be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c2e03834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Original: Good job!👍I’m so happy😊\n",
      "\n",
      "🟢 Preprocessed: Good job! thumbs up I’m so happy smiling face with smiling eyes \n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import regex as re\n",
    "\n",
    "text = \"Good job!👍I’m so happy😊\"\n",
    "emoji_text = emoji.demojize(text)\n",
    "\n",
    "# Remove ':' and '_' characters\n",
    "emoji_text_cleaned = emoji_text.replace(':', ' ').replace('_', ' ')\n",
    "\n",
    "print(f\"🔵 Original: {text}\\n\")\n",
    "print(f\"🟢 Preprocessed: {emoji_text_cleaned}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee66ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f43ef",
   "metadata": {},
   "source": [
    "## 9️⃣ **Tokenization**\n",
    "\n",
    "### ⚠️ **Very Important Step**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "Machine learning models in NLP typically require numerical input. Tokenization converts text into a format that can be converted into numerical representations (like word embeddings) that models can understand.\n",
    "\n",
    "#### **Example:**\n",
    "Let's say we have the sentence: `The quick brown fox jumps over the lazy dog.`\n",
    "Word Tokenization:\n",
    "> [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- In the search engine, \"best Italian restaurants near me\" is tokenized into the words: [\"best\", \"Italian\", \"restaurants\", \"near\", \"me\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenization splits sentences into words.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"🔵 Original:\", text)\n",
    "print(\"🟢 Preprocessed:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b63df1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e6015",
   "metadata": {},
   "source": [
    "## 🔟 **Stemming**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Stemming reduces words to a base/root form. It’s fast and works well in information retrieval systems.\n",
    "\n",
    "- May produce non-words (e.g., “studies” → “studi”).\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- Used in search engines (e.g., “searching”, “searched”, “searches” → “search”) to show relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b3186",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "text = \"We are studying various stemmed words: running, studies, cries\"\n",
    "tokens = word_tokenize(text)\n",
    "stemmed = [stemmer.stem(w) for w in tokens]\n",
    "\n",
    "print(\"🔵 Original:\", tokens)\n",
    "print(\"🟢 Preprocessed:\", stemmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8fc55b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c15c0",
   "metadata": {},
   "source": [
    "## 🔢 **Lemmatization**\n",
    "\n",
    "### 🔍 **Why:**\n",
    "- Like stemming, but linguistically accurate.\n",
    "\n",
    "- Uses vocabulary and grammar rules to return the correct base word.\n",
    "\n",
    "### 📌 **Real-World:**\n",
    "- For grammar-sensitive tasks like question answering or summarization, “was” should be lemmatized to “be”, not “wa”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98194606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"The children were playing with their better toys.\"\n",
    "doc = nlp(text)\n",
    "lemmatized = [token.lemma_ for token in doc]\n",
    "\n",
    "print(\"🔵 Original:\", text)\n",
    "print(\"🟢 Preprocessed:\", lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042e66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d646831",
   "metadata": {},
   "source": [
    "## ✅ Summary Table\n",
    "\n",
    "| Step                | Purpose                                                                 | Use Case Examples                                     |\n",
    "|---------------------|-------------------------------------------------------------------------|--------------------------------------------------------|\n",
    "| Lowercasing          | Normalize text casing                                                   | Text classification, search                           |\n",
    "| Remove HTML Tags     | remove unwanted HTML tags                                                | Web scraping, email cleaning                          |\n",
    "| Remove URLs          | Remove unnecessary web links                                             | Social media, forums                                  |\n",
    "| Remove Punctuation   | Clean punctuation noise                                                  | Preprocessing for BoW/TF-IDF                          |\n",
    "| Chat Word Treatment  | Convert slang to standard English like GD to Good Night                  | Chatbots, social media analysis                       |\n",
    "| Spelling Correction  | Fix typos for vocabulary consistency                                     | User reviews, text input                              |\n",
    "| Remove Stop Words    | Focus on meaningful words                                                 | Summarization, topic modeling                         |\n",
    "| Handle Emojis        | Preserve or convert emojis based on task                                | Sentiment analysis                                    |\n",
    "| Tokenization         | Break down text into tokens                                              | All NLP tasks                                         |\n",
    "| Stemming             | Reduce word forms to base/root                                           | Search engines, topic modeling                        |\n",
    "| Lemmatization        | Get accurate root word using grammar                                     | Parsing, QA systems, deep learning models             |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
