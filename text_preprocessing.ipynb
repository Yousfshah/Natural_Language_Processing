{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8b6d61",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f888d2",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ **Lowercasing**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Words like \"Good\", \"GOOD\", and \"good\" are semantically identical but are treated as separate tokens if not lowercased.\n",
    "\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In a product review system, \"Excellent\" and \"excellent\" should be considered the same sentiment word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbb363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: Good Morning EVERYONE! Let's Start Our NLP Journey. \n",
      "\n",
      "üü¢ Preprocessed: good morning everyone! let's start our nlp journey.\n"
     ]
    }
   ],
   "source": [
    "text = \"Good Morning EVERYONE! Let's Start Our NLP Journey.\"\n",
    "lowercased = text.lower()\n",
    "\n",
    "print(\"üîµ Original:\", text, \"\\n\")\n",
    "print(\"üü¢ Preprocessed:\", lowercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a091a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f3aa0",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ **Remove HTML Tags**\n",
    "\n",
    "### üîç **Why:**\n",
    "- HTML is markup used for layout, not meaning.\n",
    "\n",
    "- If you're scraping websites (news, blogs), tags like div tag, span tag add noise.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- When extracting articles from news sites, you'll find lots of formatting tags. Models get confused by p tag, a href tag etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: <div>Hello <b>World</b>! NLP is <i>awesome</i>.</div> \n",
      "\n",
      "üü¢ Preprocessed with BS4: Hello World! NLP is awesome. \n",
      "\n",
      "üü¢ Preprocessed with regex: Hello World! NLP is awesome.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# sample HTML text\n",
    "text = \"<div>Hello <b>World</b>! NLP is <i>awesome</i>.</div>\"\n",
    "\n",
    "# Using BeautifulSoup to remove HTML tags\n",
    "cleaned = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# Using regex to remove HTML tags\n",
    "re_pattern = \"<(?:\\\"[^\\\"]*\\\"['\\\"]*|'[^']*'['\\\"]*|[^'\\\">])+>\" # # Remove all HTML tags from a string\n",
    "cleaned_re = re.sub(re_pattern, '', text)\n",
    "# cleaned = cleaned.strip()\n",
    "\n",
    "print(\"üîµ Original:\", text, \"\\n\")\n",
    "print(\"üü¢ Preprocessed with BS4:\", cleaned, \"\\n\")\n",
    "print(\"üü¢ Preprocessed with regex:\", cleaned_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f8ff7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316925a",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ **Remove URLs**\n",
    "\n",
    "### üîç **Why:**\n",
    "- URLs usually don‚Äôt carry semantic meaning.\n",
    "\n",
    "- They are high variance strings (each one is unique) and hurt model generalization.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In a tweet like ‚ÄúCheck this out üëâ https://xyz.com‚Äù, we care more about the sentiment or emotion, not the URL itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0ffdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: Visit our website at https://www.example.com/page.html or check out www.another-site.org. You can also find info at mydomain.net/info. \n",
      "\n",
      "üü¢ Preprocessed: Visit our website at  or check out  You can also find info at \n"
     ]
    }
   ],
   "source": [
    "# it will remove any type of URL from the text just apply on your text\n",
    "import re\n",
    "\n",
    "# Example usage:\n",
    "text = \"Visit our website at https://www.example.com/page.html or check out www.another-site.org. You can also find info at mydomain.net/info.\"\n",
    "\n",
    "# Regex pattern to match various URL formats (http, https, www, without scheme)\n",
    "url_pattern = re.compile(\n",
    "    r'https?://[^\\s/$.?#].[^\\s]*'  # Matches http/https URLs\n",
    "    r'|www\\.[^\\s/$.?#].[^\\s]*'    # Matches www. URLs\n",
    "    r'|[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}(?:/[^\\s]*)?' # Matches domain-only URLs like example.com\n",
    ")\n",
    "\n",
    "# Remove URLs from the text\n",
    "text_without_urls= url_pattern.sub(r'', text)\n",
    "\n",
    "print(\"üîµ Original:\", text, \"\\n\")\n",
    "print(\"üü¢ Preprocessed:\", text_without_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d70b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752634f5",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ **Remove Punctuation**\n",
    "\n",
    "### üîç **Why:**\n",
    "-  Punctuation marks like `!`, `.`, `?` create unnecessary tokens.\n",
    "\n",
    "- Removing them helps reduce noise, especially in tasks like text classification or topic modeling.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- For spam detection or document classification, punctuation doesn‚Äôt usually help (unless you‚Äôre analyzing writing style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b850b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: (Hello!!! How are you??? I'm fine :) \n",
      "\n",
      "üü¢ Preprocessed: Hello How are you Im fine \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = \"Hello!!! How are you??? I'm fine :)\"\n",
    "no_punc = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "print(\"üîµ Original:\", text, \"\\n\")\n",
    "print(\"üü¢ Preprocessed:\", no_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e21be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5094c72",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ **Chat Word Treatment (e.g., GN ‚Üí Good Night)**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Chat language is full of abbreviations: \"lol\", \"idk\", \"smh\".\n",
    "\n",
    "- They need to be normalized to standard English for models to understand.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In customer support or social media, replacing abbreviations like `\"brb\"` with `\"be right back\"` helps understand the message better.\n",
    "\n",
    "### üó£Ô∏è **Slang Words**\n",
    "- The following two GitHub repositories provide comprehensive lists of slang words and their standard English equivalents.These resources are useful for expanding chat abbreviations (e.g., \"GN\" ‚Üí \"Good Night\") during text preprocessing\n",
    "\n",
    "    - [Repo1](https://github.com/ipekdk/abbreviation-list-english)\n",
    "    - [Repo2](https://github.com/bodhwani/NLP-VIT-BOT/blob/master/slangs.csv)\n",
    "    - [Comman Slang Words](https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55030324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: BRB guys, lol this is funny. ttyl! \n",
      "\n",
      "üü¢ Preprocessed: Be Right Back guys, Laughing Out Loud this is funny. Talk To You Later\n"
     ]
    }
   ],
   "source": [
    "# just apply to your text this comprehenisve list of slang abbreviations\n",
    "# Make sure to have a CSV file named 'slangs.csv' with columns 'Abbr\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the slang abbreviation CSV file\n",
    "chat_dict = pd.read_csv(\"slangs.csv\")\n",
    "\n",
    "# Create a mapping dictionary for fast lookup\n",
    "slang_map = dict(zip(chat_dict['Abbr'].str.lower(), chat_dict['Fullform']))\n",
    "\n",
    "text = \"BRB guys, lol this is funny. ttyl!\"\n",
    "words = text.split()\n",
    "chat_fixed = []\n",
    "for word in words:\n",
    "    key = word.lower().strip(\".,!?\")  # Remove punctuation for matching\n",
    "    if key in slang_map:\n",
    "        chat_fixed.append(slang_map[key])\n",
    "    else:\n",
    "        chat_fixed.append(word)\n",
    "\n",
    "print(\"üîµ Original:\", text, \"\\n\")\n",
    "print(\"üü¢ Preprocessed:\", ''.join(chat_fixed)) # The join() method takes all items in an iterable and joins them into one string. A string must be specified as the separator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c83a6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af2dbc",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ **Spelling Correction**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Typos are common in user input (especially social media).\n",
    "\n",
    "- Words like ‚Äúbeleive‚Äù won‚Äôt match ‚Äúbelieve‚Äù in dictionaries or embeddings.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- Spell correction boosts chatbot understanding and auto-correction in search bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0145f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: I realy love natral language prosesing. \n",
      "\n",
      "üü¢ Preprocessed (autocorrect): I really love natural language crossing. \n",
      "\n",
      "üü¢ Preprocessed (TextBlob): I really love natural language pressing. \n",
      "\n",
      "üü¢ Preprocessed (pyspellchecker): I really love natural language prosesing.\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "text = \"I realy love natral language prosesing.\"\n",
    "\n",
    "# Autocorrect\n",
    "corrected_autocorrect = spell(text)\n",
    "\n",
    "# TextBlob\n",
    "corrected_textblob = str(TextBlob(text).correct())\n",
    "\n",
    "# SpellChecker\n",
    "spellchecker = SpellChecker()\n",
    "# Tokenize the text for word-level correction\n",
    "tokens = text.split()\n",
    "corrected_pyspellchecker = ' '.join([spellchecker.correction(word) or word for word in tokens])\n",
    "\n",
    "\n",
    "print(\"üîµ Original:\", text, \"\\n\")\n",
    "print(\"üü¢ Preprocessed (autocorrect):\", corrected_autocorrect, \"\\n\")\n",
    "print(\"üü¢ Preprocessed (TextBlob):\", corrected_textblob, \"\\n\")\n",
    "print(\"üü¢ Preprocessed (pyspellchecker):\", corrected_pyspellchecker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e06efd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f78e50",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ **Remove Stop Words**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Stopwords like \"the\", \"is\", \"at\" are function words, not content words.\n",
    "\n",
    "- Removing them improves focus on important tokens.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In a movie review, ‚ÄúThe movie was absolutely fantastic‚Äù, word ‚Äúfantastic‚Äù carries the sentiment, not ‚Äúthe‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b648d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original: This is an example of removing common stop words.\n",
      "üü¢ NLTK (no stopwords): realy love natral language prosesing.\n",
      "üü¢ spaCy (no stopwords): example removing common stop words .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "# NLTK stopword removal\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "nltk_filtered = [word for word in tokens if word.lower() not in nltk_stopwords]\n",
    "nltk_no_stopwords = \" \".join(nltk_filtered)\n",
    "\n",
    "# spaCy stopword removal\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_doc = nlp(text)\n",
    "spacy_filtered = [token.text for token in spacy_doc if not token.is_stop]\n",
    "spacy_no_stopwords = \" \".join(spacy_filtered)\n",
    "\n",
    "print(\"üîµ Original:\", text)\n",
    "print(\"üü¢ NLTK (no stopwords):\", nltk_no_stopwords)\n",
    "print(\"üü¢ spaCy (no stopwords):\", spacy_no_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dc1e6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english') # You can specify other languages\n",
    "type(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61979021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yousuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Yousuf/nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Yousuf\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Download the punkt tokenizer models if not already present\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# The variables stop_words and text are already defined in the notebook\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m words = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenize the text into words\u001b[39;00m\n\u001b[32m      8\u001b[39m filtered_words = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word.lower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[32m      9\u001b[39m filtered_text = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(filtered_words)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Yousuf/nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Yousuf\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download the punkt tokenizer models if not already present\n",
    "\n",
    "# The variables stop_words and text are already defined in the notebook\n",
    "\n",
    "words = word_tokenize(text)  # Tokenize the text into words\n",
    "\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "filtered_text = \" \".join(filtered_words)\n",
    "\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f2c8dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yousuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yousuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Yousuf/nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Yousuf\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mThis is an example of removing common stop words.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m filtered = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word.lower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m     12\u001b[39m no_stopwords = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(filtered)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yousuf\\miniconda3\\envs\\NLP-Env\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Yousuf/nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yousuf\\\\miniconda3\\\\envs\\\\NLP-Env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Yousuf\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Ensure required NLTK resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"This is an example of removing common stop words.\"\n",
    "tokens = word_tokenize(text)\n",
    "filtered = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "no_stopwords = \" \".join(filtered)\n",
    "\n",
    "print(\"üîµ Original:\", text)\n",
    "print(\"üü¢ Preprocessed:\", no_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd6e4b1",
   "metadata": {},
   "source": [
    "| Feature       | NLTK                             | spaCy                                   |\n",
    "| ------------- | -------------------------------- | --------------------------------------- |\n",
    "| Stopword List | Manually maintained              | Linguistically curated                  |\n",
    "| Tokenization  | Basic (word\\_tokenize)           | Advanced (handles context, punctuation) |\n",
    "| Flexibility   | High (can customize list easily) | High (custom rules, pipelines)          |\n",
    "| Performance   | Lightweight                      | Heavier but more powerful               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc3974",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680edd1",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ **Handling Emojis**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Emojis are sentiment-rich tokens (üòä, üò¢).\n",
    "\n",
    "- You can either remove them, convert to words, or use them as features.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In social media sentiment analysis, üò† and ‚ù§Ô∏è change the tone completely and must be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e03834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "text = \"Good job! üëç I‚Äôm so happy üòä\"\n",
    "emoji_text = emoji.demojize(text)\n",
    "\n",
    "print(\"üîµ Original:\", text)\n",
    "print(\"üü¢ Preprocessed:\", emoji_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee66ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f43ef",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ **Tokenization**\n",
    "\n",
    "### ‚ö†Ô∏è **Very Important Step**\n",
    "\n",
    "### üîç **Why:**\n",
    "Machine learning models in NLP typically require numerical input. Tokenization converts text into a format that can be converted into numerical representations (like word embeddings) that models can understand.\n",
    "\n",
    "#### **Example:**\n",
    "Let's say we have the sentence: `The quick brown fox jumps over the lazy dog.`\n",
    "Word Tokenization:\n",
    "> [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- In the search engine, \"best Italian restaurants near me\" is tokenized into the words: [\"best\", \"Italian\", \"restaurants\", \"near\", \"me\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenization splits sentences into words.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"üîµ Original:\", text)\n",
    "print(\"üü¢ Preprocessed:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b63df1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e6015",
   "metadata": {},
   "source": [
    "## üîü **Stemming**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Stemming reduces words to a base/root form. It‚Äôs fast and works well in information retrieval systems.\n",
    "\n",
    "- May produce non-words (e.g., ‚Äústudies‚Äù ‚Üí ‚Äústudi‚Äù).\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- Used in search engines (e.g., ‚Äúsearching‚Äù, ‚Äúsearched‚Äù, ‚Äúsearches‚Äù ‚Üí ‚Äúsearch‚Äù) to show relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b3186",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "text = \"We are studying various stemmed words: running, studies, cries\"\n",
    "tokens = word_tokenize(text)\n",
    "stemmed = [stemmer.stem(w) for w in tokens]\n",
    "\n",
    "print(\"üîµ Original:\", tokens)\n",
    "print(\"üü¢ Preprocessed:\", stemmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8fc55b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c15c0",
   "metadata": {},
   "source": [
    "## üî¢ **Lemmatization**\n",
    "\n",
    "### üîç **Why:**\n",
    "- Like stemming, but linguistically accurate.\n",
    "\n",
    "- Uses vocabulary and grammar rules to return the correct base word.\n",
    "\n",
    "### üìå **Real-World:**\n",
    "- For grammar-sensitive tasks like question answering or summarization, ‚Äúwas‚Äù should be lemmatized to ‚Äúbe‚Äù, not ‚Äúwa‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98194606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"The children were playing with their better toys.\"\n",
    "doc = nlp(text)\n",
    "lemmatized = [token.lemma_ for token in doc]\n",
    "\n",
    "print(\"üîµ Original:\", text)\n",
    "print(\"üü¢ Preprocessed:\", lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042e66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d646831",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary Table\n",
    "\n",
    "| Step                | Purpose                                                                 | Use Case Examples                                     |\n",
    "|---------------------|-------------------------------------------------------------------------|--------------------------------------------------------|\n",
    "| Lowercasing          | Normalize text casing                                                   | Text classification, search                           |\n",
    "| Remove HTML Tags     | remove unwanted HTML tags                                                | Web scraping, email cleaning                          |\n",
    "| Remove URLs          | Remove unnecessary web links                                             | Social media, forums                                  |\n",
    "| Remove Punctuation   | Clean punctuation noise                                                  | Preprocessing for BoW/TF-IDF                          |\n",
    "| Chat Word Treatment  | Convert slang to standard English like GD to Good Night                  | Chatbots, social media analysis                       |\n",
    "| Spelling Correction  | Fix typos for vocabulary consistency                                     | User reviews, text input                              |\n",
    "| Remove Stop Words    | Focus on meaningful words                                                 | Summarization, topic modeling                         |\n",
    "| Handle Emojis        | Preserve or convert emojis based on task                                | Sentiment analysis                                    |\n",
    "| Tokenization         | Break down text into tokens                                              | All NLP tasks                                         |\n",
    "| Stemming             | Reduce word forms to base/root                                           | Search engines, topic modeling                        |\n",
    "| Lemmatization        | Get accurate root word using grammar                                     | Parsing, QA systems, deep learning models             |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
